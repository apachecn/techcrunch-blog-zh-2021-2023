<html>
<head>
<title>There's now an open source alternative to ChatGPT, but good luck running it | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">现在有了 ChatGPT 的开源替代方案，但祝你好运</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/12/30/theres-now-an-open-source-alternative-to-chatgpt-but-good-luck-running-it/">https://web.archive.org/web/https://techcrunch.com/2022/12/30/theres-now-an-open-source-alternative-to-chatgpt-but-good-luck-running-it/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">第一个相当于 OpenAI 的<a href="https://web.archive.org/web/20230409032106/https://techcrunch.com/tag/chatgpt/"> ChatGPT </a>的开源软件已经到来，但是祝你在笔记本电脑上运行它好运——或者根本不运行它。</p>
<p class="translated">本周，Philip Wang，这位负责逆向工程闭源人工智能系统(包括 Meta 的 Make-A-Video T3)的开发者，发布了 PaLM + RLHF，一种行为类似于 ChatGPT 的文本生成模型。该系统结合了谷歌的大型语言模型<a href="https://web.archive.org/web/20230409032106/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" target="_blank" rel="noopener"> PaLM </a>和一种称为人类反馈强化学习(简称 RLHF)的技术，来创建一个可以完成几乎任何 ChatGPT 可以完成的任务的系统，包括起草电子邮件和建议计算机代码。</p>
<p class="translated">但是 PaLM + RLHF 不是预训的。也就是说，该系统还没有在实际工作所需的来自网络的示例数据上进行训练。下载 PaLM + RLHF 不会神奇地安装类似 ChatGPT 的体验——这需要编译模型可以学习的千兆字节的文本，并找到足够强大的硬件来处理训练工作量。</p>
<p class="translated">和 ChatGPT 一样，PaLM + RLHF 本质上是一个预测单词的统计工具。当从训练数据中获得大量例子时，例如来自 Reddit 的帖子、新闻文章和电子书，PaLM + RLHF 会根据周围文本的语义上下文等模式来学习单词出现的可能性。</p>
<p class="translated">ChatGPT 和 PaLM + RLHF 在人类反馈的强化学习方面有着特殊的优势，这种技术旨在更好地将语言模型与用户希望他们完成的事情联系起来。RLHF 涉及训练一个语言模型——在 PaLM + RLHF 的情况下是 PaLM——并在一个数据集上对其进行微调，该数据集包括提示(例如，“向一个六岁的孩子解释机器学习”)和人类志愿者期望模型说的话(例如，“机器学习是人工智能的一种形式……”)。然后，上述提示被输入到微调模型中，该模型生成几个回答，志愿者将所有回答从最好到最差进行排序。最后，排名被用来训练一个“奖励模型”，该模型接受原始模型的回答，并按照偏好排序，筛选出给定提示的最佳答案。</p>
<p class="translated">收集训练数据是一个昂贵的过程。培训本身并不便宜。PaLM 有 5400 亿个参数，“参数”指的是从训练数据中学习到的语言模型的部分。2020 年<a href="https://web.archive.org/web/20230409032106/https://arxiv.org/pdf/2004.08900.pdf" target="_blank" rel="noopener">的一项研究</a>估算出开发一个只有 15 亿个参数的文本生成模型的费用高达 160 万美元。而训练拥有 1760 亿个参数的开源模型<a href="https://web.archive.org/web/20230409032106/https://techcrunch.com/2022/07/12/a-year-in-the-making-bigsciences-ai-language-model-is-finally-available/"> Bloom </a>，用了 384 个 Nvidia A100 GPUs，耗时三个月；一个 100 就要几千美元。</p>
<p class="translated">运行 PaLM + RLHF 大小的训练模型也不是小事。<a href="https://web.archive.org/web/20230409032106/https://techcrunch.com/2022/07/12/a-year-in-the-making-bigsciences-ai-language-model-is-finally-available/">布鲁姆</a>需要一台配备大约八个 A100 图形处理器的专用电脑。云替代方案价格昂贵，粗略计算<a href="https://web.archive.org/web/20230409032106/https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/" target="_blank" rel="noopener">发现</a>在一个亚马逊网络服务实例上运行 OpenAI 的文本生成<a href="https://web.archive.org/web/20230409032106/https://techcrunch.com/tag/gpt-3/">GPT-3</a>——约有 1750 亿个参数——的成本约为每年 87，000 美元。</p><p class="piano-inline-promo"/>
<p class="translated">人工智能研究员 Sebastian Raschka 在 LinkedIn 的一篇关于 PaLM + RLHF 的帖子中指出，扩大必要的开发工作流也可能是一个挑战。“即使有人为你提供 500 个 GPU 来训练这个模型，你仍然需要处理基础设施，并拥有一个可以处理这些的软件框架，”他说。“这显然是可能的，但目前这是一项巨大的努力(当然，我们正在开发框架以使其更简单，但它仍然不是微不足道的。”</p>
<p class="translated">这就是说，PaLM + RLHF 今天不会取代 ChatGPT 除非一个资金雄厚的企业(或个人)不怕麻烦地培训并公开发布它。</p>
<p class="translated">好消息是，其他几项复制 ChatGPT 的努力进展迅速，其中包括由一个名为 CarperAI 的研究小组领导的一项努力。CarperAI 与开放的人工智能研究组织 EleutherAI 和初创公司 Scale AI 和 Hugging Face 合作，计划发布第一个可以运行的、类似 ChatGPT 的人工智能模型，通过人类反馈进行训练。</p>
<p class="translated">LAION 是一家非营利组织，它提供了用于训练稳定扩散的初始数据集，也是 T2 的先锋项目，使用最新的机器学习技术复制 ChatGPT。LAION 雄心勃勃，旨在建立一个“未来的助手”——不仅写电子邮件和求职信，而且“做有意义的工作，使用 API，动态研究信息等等。”它处于早期阶段。但是几周前，GitHub <a href="https://web.archive.org/web/20230409032106/https://github.com/LAION-AI/Open-Assistant" target="_blank" rel="noopener">页面</a>上线了该项目的资源。</p>
			</div>

			</div>    
</body>
</html>