<html>
<head>
<title>Who's liable for AI-generated lies? | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谁该为人工智能制造的谎言负责？TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/06/01/whos-liable-for-ai-generated-lies/">https://web.archive.org/web/https://techcrunch.com/2022/06/01/whos-liable-for-ai-generated-lies/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><span class="featured__span-first-words">谁将为大型语言模型产生的有害言论承担责任</span>？随着 OpenAI 的 GPT-3 等先进人工智能因在自然语言处理和生成方面取得令人印象深刻的突破而受到欢呼——从更流畅的文案到更强大的客户服务聊天机器人，该技术的各种(生产性)应用都在设想之中——这些强大的文本生成工具无意中自动滥用和传播诽谤的风险不容忽视。坏人故意将技术武器化以传播混乱、扩大危害并看着世界燃烧的风险也不会存在。</p>
<p class="translated">事实上，OpenAI 非常担心其模型“完全偏离轨道”的风险，正如其<a href="https://web.archive.org/web/20230408023132/https://beta.openai.com/docs/engines/content-filter" target="_blank" rel="noopener">文档中所说的</a>(参考一个响应示例，其中滥用的客户输入得到了非常 troll 式的人工智能回复)，以提供一个免费的内容过滤器，“旨在检测来自 API 的可能敏感或不安全的生成文本”，并建议用户不要返回任何过滤器认为“不安全”的生成文本(明确地说，它的文档将“不安全”定义为“文本包含亵渎性语言、偏见或仇恨语言，可能是 NSFW 或以有害方式描绘某些群体/人的文本。”).</p>
<p class="translated">但是，考虑到这项技术的新颖性，没有明确的法律要求必须应用内容过滤器。因此，OpenAI 要么是出于避免其模型对人类造成生殖伤害的担忧——和/或声誉担忧——因为如果该技术与即时毒性相关联，可能会阻碍发展。</p>
<p class="translated">回想一下微软命运多舛的<a href="https://web.archive.org/web/20230408023132/https://twitter.com/TayandYou" target="_blank" rel="noopener"> Tay AI Twitter 聊天机器人</a>，它于 2016 年 3 月大张旗鼓地推出，该公司的研究团队称之为“对话理解”的实验然而，在网络用户“教会”这个机器人宣扬种族主义、反犹太主义和厌恶女性的仇恨言论后，不到一天，微软就拔掉了它的插头。因此，它结束了一种不同的实验:在线文化如何传导和放大人类可能有的最糟糕的冲动。</p>
<p class="translated">同样的底层互联网内容也被吸进了今天的大型语言模型——因为人工智能模型构建者已经在互联网上爬来爬去，以获得他们需要的大量自由文本，来训练和提高他们的语言生成能力。(例如，根据维基百科的说法，OpenAI 的 GPT-3 的加权预训练数据集的 60%来自普通爬行的过滤版本——也就是由抓取的网络数据组成的免费数据集。)这意味着这些强大得多的大型语言模型可能会陷入讽刺性的套话，甚至更糟。</p>
<p class="translated">欧洲政策制定者几乎没有解决如何在当前背景下<a href="https://web.archive.org/web/20230408023132/https://techcrunch.com/2022/01/24/dcms-committee-report-on-online-safety-bill/">监管在线危害</a>，比如通过算法排序的社交媒体平台，其中大部分言论至少可以追溯到人类——更不用说考虑人工智能驱动的文本生成如何加剧在线毒性问题，同时围绕责任创造新的困境。</p>
<p class="translated">如果没有明确的责任，很可能更难防止人工智能系统被用来扩大语言伤害。</p><p class="piano-inline-promo"/>
<p class="translated">以诽谤为例。法律已经面临挑战，应对自动生成的完全错误的内容。</p>
<p class="translated">安全研究员 Marcus Hutchins 几个月前去了抖音，向他的追随者展示他是如何“被谷歌的人工智能欺负的”，正如<a href="https://web.archive.org/web/20230408023132/https://www.tiktok.com/@malwaretech/video/7085169223546375470?_r=1&amp;_t=8ROFUXK3PJe&amp;social_sharing=v3" target="_blank" rel="noopener">所说</a>。在一个非常爽朗的片段中，考虑到他正在解释一个卡夫卡式的噩梦，其中世界上最有价值的公司之一不断发布关于他的诽谤性建议，哈钦斯解释说，如果你在谷歌上搜索他的名字，它返回的搜索引擎结果页面(SERP)包括一个自动生成的 Q&amp;A——谷歌在其中错误地声明哈钦斯制造了 WannaCry 病毒。</p>
<p class="translated">事实上，哈钦斯以<em>阻止</em> WannaCry 闻名。然而，谷歌的人工智能在这个一点也不棘手的区分本质差异的问题上抓住了错误的一端——而且似乎一直在犯错。反反复复。(大概是因为很多在线文章在提到“WannaCry”的同时提到了 Hutchins 的名字——但这是因为他阻止了全球勒索软件攻击变得更加糟糕。所以这是谷歌的一些真正的人为愚蠢行为。)</p>
<p class="translated">以至于哈钦斯说，他几乎放弃了通过修复其失灵的人工智能来让公司停止诽谤他的努力。</p>
<p class="translated">哈钦斯告诉 TechCrunch:“让这个问题变得如此困难的主要问题是，虽然在 Twitter 上引起足够的关注就可以解决几个问题，但由于整个系统是自动化的，它只会在以后添加更多，就像玩打地鼠一样。”。“已经到了我不能再提出这个问题的地步，因为我听起来就像一张破唱片，人们会感到恼火。”</p>
<p class="translated">在我们向谷歌询问这个错误的 SERP 后的几个月里，与 Hutchins 相关的问答发生了变化——所以不再问“Marcus Hutchins 制造了什么病毒？”——在一篇新闻报道的较长文本片段中提供(正确的)上下文之前，直接在下面浮现一个单词(不正确的)答案:“WannaCry”，就像今年 4 月一样，搜索 Hutchins 的名字，现在谷歌会显示问题“谁创建了 WannaCry”(见下面的截屏)。但现在它没有回答自己的问题——因为它下面显示的文本片段只谈到了 Hutchins 阻止病毒传播。</p>
<p/><div id="attachment_2328046" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2328046" decoding="async" class="breakout wp-image-2328046 size-full" src="../Images/c97c4961cb70f1eed442fbcdfcfe5d57.png" alt="" srcset="https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png 1346w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=150,34 150w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=300,68 300w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=768,175 768w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=680,155 680w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=1200,273 1200w, https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png?resize=50,11 50w" sizes="(max-width: 1346px) 100vw, 1346px" data-original-src="https://web.archive.org/web/20230408023132im_/https://techcrunch.com/wp-content/uploads/2022/06/Screenshot-2022-06-01-at-15.49.41.png"/><p id="caption-attachment-2328046" class="wp-caption-text translated">图片来源:娜塔莎·洛马斯/TechCrunch(截屏)</p></div>
<p class="translated">因此，我们必须假设，谷歌已经调整了人工智能显示这个 SERP 的问答格式。但是这样做破坏了格式(因为它提出的问题从来没有得到回答)。</p>
<p class="translated">此外，误导性的介绍配对的问题“谁创造了 WannaCry？”搜索 Hutchins 的名字，仍然可能导致快速浏览谷歌在问题后显示的文本的网络用户错误地认为他是该病毒的作者。所以不清楚它是否比以前自动生成的东西有很大/任何改进。</p>
<p class="translated">在早些时候对 TechCrunch 的评论中，哈钦斯还指出，问题本身的背景，以及谷歌搜索结果的方式，可能会造成他制造病毒的误导印象——他补充说:“当有人在谷歌上搜索一个学校项目时，他们不太可能会阅读整篇文章，因为他们觉得答案就在那里。”</p>
<p class="translated">他还将谷歌自动生成的文本与直接的人身伤害联系起来，告诉我们:“自从谷歌开始展示这些 SERPs，我收到了大量仇恨评论，甚至是基于我创建 WannaCry 的威胁。我的法律案件的时间给人的印象是，联邦调查局怀疑我，但快速(谷歌搜索)将证实事实并非如此。现在有各种各样的 SERP 结果暗示我做了，证实了搜索者的怀疑，这给我造成了相当大的损害。”</p>
<p class="translated">在被要求对他的投诉做出回应时，谷歌给我们发来了一份声明，据称是一位发言人写的:</p>
<blockquote><p class="translated">此功能中的查询是自动生成的，旨在突出显示其他常见的相关搜索。我们有适当的系统来防止不正确或无用的内容出现在这个功能。总的来说，我们的系统工作得很好，但是它们没有对人类语言的完美理解。当我们意识到搜索功能中的内容<a href="https://web.archive.org/web/20230408023132/https://support.google.com/websearch/answer/10622781#features_policies" target="_blank" rel="noopener" data-saferedirecturl="https://www.google.com/url?q=https://support.google.com/websearch/answer/10622781%23features_policies&amp;source=gmail&amp;ust=1654178997484000&amp;usg=AOvVaw0OEVAhd1ud7JN5KUWPVSeI">违反了我们的政策</a>时，我们会迅速采取行动，就像我们在这个案例中所做的那样。</p></blockquote>
<p class="translated">这家科技巨头没有回应后续问题，指出其“行动”一直未能解决 Hutchins 的投诉。</p>
<p class="translated">这当然只是一个例子——但它看起来很有启发性，一个人，拥有相对较大的在线存在和平台来放大他对谷歌“欺凌人工智能”的抱怨，实际上无法阻止该公司应用自动化技术，不断出现和重复对他的诽谤性建议。</p>
<p class="translated">在他的抖音视频中，哈钦斯表示在美国没有就这个问题起诉谷歌的追索权——他说这“本质上是因为人工智能在法律上不是一个人，没有人需要承担法律责任；不能认为是诽谤或中伤。”</p>
<p class="translated">诽谤法因你投诉的国家而异。如果哈钦斯在德国等欧洲市场提起诉讼，他很有可能获得法院下令修复该 SERP 的更大机会——谷歌此前曾在德国被<a href="https://web.archive.org/web/20230408023132/https://techcrunch.com/2012/09/07/germanys-former-first-lady-sues-google-for-defamation-over-autocomplete-suggestions/">起诉诽谤自动完成搜索建议</a>(尽管那次法律行动的结果是，由贝蒂娜·武尔夫、 不太清楚，但似乎她投诉的虚假自动完成建议被谷歌的技术链接到她的名字确实得到了修复)——而不是在美国，美国《通信得体法》第 230 条为平台提供了对第三方内容责任的一般豁免。</p>
<p class="translated">虽然，在 Hutchins SERP 的案例中，<em>的问题，也就是</em>的内容，是一个关键的考虑因素。谷歌可能会争辩说，它的人工智能只是反映了其他人以前发表的东西——因此，Q &amp; A 应该受到 230 条款的豁免。但或许可以(反驳)认为，人工智能对文本的选择和呈现相当于一种实质性的重新混合，这意味着语音——或者至少是上下文——实际上是由谷歌生成的。那么，这家科技巨头真的应该为其人工智能生成的文本安排享有免责保护吗？</p>
<p class="translated">对于大型语言模型来说，模型制作者肯定会越来越难质疑它们的人工智能正在生成语音。但个人投诉和诉讼看起来不像是对可能成为大规模自动化诽谤(和滥用)的可扩展修复——这要归功于这些大型语言模型的能力增强，以及随着 API 的开放而扩大的访问范围。</p>
<p class="translated">监管机构将需要解决这个问题——以及人工智能产生的通信的责任在哪里。这意味着应对分配责任的复杂性，考虑到有多少实体可能参与应用和迭代大型语言模型，以及塑造和分发这些人工智能系统的输出。</p>
<p class="translated">在欧盟，地区立法者走在监管曲线的前面，因为他们目前正在努力制定基于风险的框架的细节 <span>委员会去年提议<a href="https://web.archive.org/web/20230408023132/https://techcrunch.com/2021/04/21/europe-lays-out-plan-for-risk-based-ai-rules-to-boost-trust-and-uptake/"/>为人工智能的某些应用制定规则，以确保高度可扩展的自动化技术以安全和非歧视的方式应用。</span></p>
<p class="translated">但是目前还不清楚欧盟起草的人工智能法案是否会对大型语言模型的恶意和/或鲁莽应用提供足够的检查和平衡，因为它们被归类为通用人工智能系统，不在最初的委员会草案中。</p>
<p class="translated"><span>该法案本身制定了一个框架，定义了有限的一组“高风险”人工智能应用类别，如就业、执法、生物识别 ID 等，其中提供商具有最高级别的合规性要求。但是一个大型语言模型输出的下游应用者——他们可能依赖于 API 来将能力传输到他们特定的领域用例中——不太可能有必要的访问权(对训练数据等)。)能够理解模型的稳健性或它可能带来的风险；或者进行更改以减轻他们遇到的任何问题，例如通过用不同的数据集重新训练模型。</span> <span> </span></p>
<p class="translated">欧洲的法律专家和公民社会团体对通用人工智能的这种划分提出了担忧。在最近的部分妥协文本中，在共同立法者的讨论中出现了一篇关于通用人工智能系统的文章。但是，上个月，两个民间社会团体在 euractiv 上撰文警告说，建议的妥协将为通用人工智能的制造商创造一个持续的例外——通过将所有的责任都放在使用系统的部署者身上，这些系统的工作情况他们默认是不知道的。</p>
<p class="translated">“许多数据治理要求，特别是偏差监控、检测和纠正，需要访问人工智能系统训练的数据集。然而，这些数据集归开发者所有，而不归用户所有，用户将通用人工智能系统“用于预期目的”因此，对于这些系统的用户来说，根本不可能满足这些数据治理要求，”他们警告说。</p>
<p class="translated">我们就此采访了一位法律专家，互联网法律学者 Lilian Edwards——他之前曾<a href="https://web.archive.org/web/20230408023132/https://techcrunch.com/2022/04/01/ai-act-powers/">批评过欧盟框架</a>的许多限制——他说，对大型上游通用人工智能系统的提供商提出一些要求的提议是向前迈出的一步。但她表示，执行起来似乎很困难。尽管她欢迎增加一项要求的提议，即大型语言模型等人工智能系统的提供商必须根据最新的妥协文本，向下游部署者“提供必要的信息”,但她指出，还建议对知识产权或机密商业信息/商业秘密进行豁免——这有可能致命地破坏整个义务。</p>
<p class="translated">所以，TL；DR:即使是欧洲监管人工智能应用的旗舰框架，也仍然有一段路要走，以抓住人工智能的前沿——如果它不辜负宣传的可信、尊重、以人为中心的人工智能蓝图，它必须做到这一点。否则，一系列技术加速的伤害看起来几乎是不可避免的——为在线文化战争提供无限的燃料(垃圾邮件级别的按钮式钓鱼、虐待、仇恨言论、虚假信息！)——并建立了一个黯淡的未来，目标个人和团体只能去扑灭永无止境的仇恨和谎言。这是公平的反义词。</p>
<p class="translated">欧盟近年来在很大程度上提高了数字立法的速度，但在涉及人工智能系统时，欧盟的立法者必须跳出现有产品规则的框框，如果他们要为快速发展的自动化技术设置有意义的护栏，并避免让主要参与者不断回避其社会责任的漏洞。没有人应该获得自动化伤害的通行证——无论“黑盒”学习系统位于链中的哪个位置，也无论用户是大是小——否则将是我们人类拿着一面黑暗的镜子。</p>

			</div>

			</div>    
</body>
</html>