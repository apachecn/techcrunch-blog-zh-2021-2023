<html>
<head>
<title>The Great Pretender | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伟大的伪装者</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2023/04/03/the-great-pretender/">https://web.archive.org/web/https://techcrunch.com/2023/04/03/the-great-pretender/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><span class="featured__span-first-words">有一个很好的</span>理由不相信<a href="https://web.archive.org/web/20230408090840/https://techcrunch.com/2023/03/21/googles-bard-lags-behind-gpt-4-and-claude-in-head-to-head-comparison/">今天的人工智能构造</a>告诉你的东西，它与智能或人类的基本性质无关，与维特根斯坦的语言表示概念无关，甚至与数据集中的反信息无关。重要的是，这些系统不能区分<em>是</em>正确的东西和<em>看起来</em>正确的东西。一旦你明白人工智能认为这些东西或多或少是可以互换的，一切都变得更有意义了。</p>
<p class="translated">现在，我并不打算回避任何关于这种在各种形式的媒体和对话中不断发生的有趣而广泛的讨论。从哲学家和语言学家到工程师和黑客，再到酒保和消防员，我们每个人都在质疑和辩论“智能”和“语言”到底是什么，以及 ChatGPT 这样的东西是否拥有它们。</p>
<p class="translated">这太神奇了！当这个领域的一些最聪明的人享受他们在阳光下的时刻时，我已经学到了很多，而从比较幼稚的人的嘴里可以获得新的观点。</p>
<p class="translated">但与此同时，当有人问“所有这些 GPT 的东西怎么样，人工智能变得有多聪明有点可怕，对吗？”时，喝啤酒或咖啡也有很多要整理的你从哪里开始——亚里士多德、土耳其机器人、感知器还是“你所需要的只是注意力”？</p>
<p class="translated">在其中一次聊天中，我发现了一个简单的方法，可以帮助人们了解为什么这些系统既很酷又完全不可信，同时丝毫不影响它们在某些领域的实用性以及它们周围正在进行的令人惊叹的对话。我想我会把它分享给你，以防你在和其他好奇的、持怀疑态度的人讨论这个问题时，发现这个观点很有用，尽管他们不想听向量或矩阵。</p>
<p class="translated">只有三件事需要理解，这就引出了一个自然的结论:</p>
<ol>
<li class="translated">这些模型是通过让他们在一个巨大的文本数据集中观察单词和句子等之间的关系，然后建立他们自己的内部统计图来描述所有这些数以百万计的单词和概念是如何关联和相关的。没有人说过，这是名词，这是动词，这是菜谱，这是修辞手法；但这些都是在使用模式中自然显现的东西。</li>
<li class="translated">这些模型没有被专门教授如何回答问题，这与过去十年来谷歌和苹果等熟悉的软件公司一直称之为人工智能形成了对比。那些<em>基本上是疯狂的库，空格指向 API:每个问题要么被解释，要么产生一个通用的响应。对于大型语言模型，问题只是一系列单词，就像其他单词一样。</em></li>
<li class="translated">这些模型在它们的响应中有一个基本的表达质量“信心”。在一个简单的猫识别人工智能的例子中，它会从 0 到 100，0 表示完全确定那不是猫，100 表示完全确定那是猫。你可以告诉它说“是的，这是一只猫”,如果它的置信度是 85，或者 90，无论产生什么样的首选响应度量。</li>
</ol>
<p class="translated">因此，鉴于我们对模型工作原理的了解，这里有一个关键问题:它对有什么信心？它不知道什么是猫或问题，只知道训练集中数据节点之间的统计关系。一个小小的调整会让 cat 检测器同样确信照片显示的是一头牛，或是天空，或是一幅静物画。模型不能对自己的“知识”有信心，因为它没有办法实际评估它被训练的数据的内容。</p>
<p class="translated"><strong>人工智能正在表达它的答案<em>在用户</em>看来是正确的有多确定。</strong></p>
<p class="translated">cat 探测器如此，GPT 4 号也是如此——区别在于输出的长度和复杂性。人工智能不能区分正确和错误的答案——它只能预测一系列单词有多大可能被认为是正确的。这就是为什么它必须被认为是世界上信息最全面的胡扯，而不是任何主题的权威。它甚至不知道它在骗你— <strong>它已经被训练产生一个<em>统计上与正确答案</em>相似的反应，它会说<em>任何事情</em>来改善这种相似性。</strong></p>
<p class="translated">人工智能不知道任何问题的答案，因为它不理解这个问题。它不知道什么是问题。它什么都不“知道”！答案跟在问题后面，因为根据统计分析推断，这一系列单词最有可能跟在前面的一系列单词后面。这些词是否指真实的地方、人、地点等。并不重要——只是它们像真的一样。</p>
<p class="translated">这与人工智能可以创作出一幅类似莫奈的画而不是莫奈的画的原因是一样的——重要的是，它具有导致人们将一件艺术品识别为他的作品的所有特征。今天的人工智能接近事实反应，就像它接近“睡莲”一样</p>
<p class="translated">现在，我赶紧补充一点，这不是一个原创或突破性的概念——它基本上是解释随机鹦鹉或海底章鱼的另一种方式。非常聪明的人很早就发现了这些问题，这也是广泛阅读科技评论的一个很好的理由。</p>

<p class="translated">但在今天的聊天机器人系统的背景下，我刚刚发现人们直觉地获得了这种方法:模型不理解事实或概念，而是单词之间的关系，它的响应是答案的“艺术家印象”。当你认真对待时，他们的目标是令人信服地填补空白<em/>，而不是正确地<em/>。这就是为什么它的回答根本不可信的原因。</p>
<p class="translated">当然有时候，甚至很多时候，它的答案<em>是</em>正确！这并非偶然:对于许多问题来说，看起来最正确的答案就是正确的答案。这就是这些模型如此强大——也如此危险的原因。从对数百万个单词和文件的系统研究中，你可以提取出如此之多的东西。与完全再现“睡莲”不同，语言具有灵活性，可以让近似的真实反应也是真实的——但也可以让完全或部分虚构的反应看起来一样或更真实。人工智能唯一关心的是答案扫描正确。</p>
<p class="translated">这为围绕这是否是真正的知识、模型是否“理解”了什么、它们是否获得了某种形式的智能、智能究竟是什么等等的讨论打开了大门。带上维特根斯坦！</p>
<p class="translated">此外，它也为在不关心真相的情况下使用这些工具留下了可能性。如果你想为一个开篇段落生成五种变体以避免文思枯竭，人工智能可能是必不可少的。如果你想编一个关于两种濒危动物的故事，或者写一首关于神奇宝贝的十四行诗，那就去写吧。只要响应反映现实并不重要，大型语言模型就是一个愿意且有能力的合作伙伴——并非巧合的是，这也是人们最感兴趣的地方。</p>
<p class="translated">人工智能何时何地出错是非常非常难以预测的，因为模型太大而且不透明。想象一下，一张有一个大陆那么大的卡片目录，由机器人根据他们在飞行中想出的基本原则，在一百多年的时间里进行组织和更新。你认为你可以走进去理解这个系统？它对一个难题给出了正确的答案，对一个简单的问题给出了错误的答案。为什么？现在这是一个人工智能和它的创造者都无法回答的问题。</p>
<p class="translated">这在未来很可能会改变，甚至在不久的将来。一切都在快速而不可预测地变化，没有什么是确定的。但就目前而言，这是一个需要记住的有用的心理模型:人工智能希望你相信它，并会说任何话来增加它的机会。</p>
			</div>

			</div>    
</body>
</html>