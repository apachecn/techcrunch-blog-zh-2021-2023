<html>
<head>
<title>While anticipation builds for GPT-4, OpenAI quietly releases GPT-3.5 | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当对 GPT 4 的预期建立时，OpenAI 悄悄地发布了 GPT 3.5 | TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/12/01/while-anticipation-builds-for-gpt-4-openai-quietly-releases-gpt-3-5/">https://web.archive.org/web/https://techcrunch.com/2022/12/01/while-anticipation-builds-for-gpt-4-openai-quietly-releases-gpt-3-5/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">两年前发布的 OpenAI 非常有能力，如果有缺陷的话，GPT-3 可能是第一个证明 AI 可以像人类一样令人信服地写作的人。GPT-3 的继任者，最有可能被称为 GPT-4，预计将在不久的将来亮相，最早可能在 2023 年。但与此同时，OpenAI 已经悄悄地推出了一系列基于“GPT-3.5”的人工智能模型，这是 GPT-3 之前未经宣布的改进版本。</p>
<p class="translated">GPT 3.5 在周三突破了 ChatGPT 的覆盖，ChatGPT 是 GPT 3.5 的一个微调版本，本质上是一个通用的聊天机器人。在昨天下午的<a href="https://web.archive.org/web/20230408022140/https://chat.openai.com/chat">公开演示</a>中首次亮相，ChatGPT 可以涉及一系列话题，包括节目、电视脚本和科学概念。</p>
<p class="translated"><a href="https://web.archive.org/web/20230408022140/https://beta.openai.com/docs/model-index-for-researchers">根据 OpenAI 的</a>，GPT-3.5 接受了 2021 年第 4 季度之前发布的文本和代码混合的培训。像 GPT-3 和其他文本生成人工智能一样，GPT-3.5 通过从网络上吸收大量内容，包括成千上万的维基百科条目、社交媒体帖子和新闻文章，学会了句子、单词和单词部分之间的关系。</p>
<p class="translated">OpenAI 没有发布经过充分训练的 GPT-3.5，而是使用它来创建几个针对特定任务进行微调的系统——每个系统都可以通过 OpenAI 应用编程接口获得。根据实验室的说法，一种——text-davici-003——可以处理比建立在 GPT-3 上的模型更复杂的指令，而且在长格式和“高质量”写作方面都明显更好。</p>
<p class="translated">根据 OpenAI 数据科学家简·雷科的说法，text-davinci-003 类似于 InstructGPT，但并不完全相同，instructGPT 是 OpenAI 今年早些时候发布的一系列基于 GPT 3 的模型，不太可能生成有问题的(如<a href="https://web.archive.org/web/20230408022140/https://techcrunch.com/2020/08/07/here-are-a-few-ways-gpt-3-can-go-wrong/">有毒且高度偏颇的</a>)文本，同时更符合用户的意图。雷科在推特上说，text-davici-003——以及延伸的 GPT-3.5——在遭受“不太严重的”限制的同时，“在人类偏好评级上得分更高”。</p>

<p class="translated">有趣的是，情况似乎就是这样。内容营销平台 Pepper Content 的数据科学家<a href="https://web.archive.org/web/20230408022140/https://www.peppercontent.io/blog/revolutionizing-text-processing-with-text-davinci-003/">报告</a>称，text-davinci-003“在理解请求背后的‘上下文’方面表现更好，然后利用它来产生更好的内容”，而“幻觉”比基于 GPT 3 的模型更少。(在涉及生成文本的人工智能时，<em>幻觉</em>指的是人工智能写下不一致、事实上不正确的陈述。)</p>
<p class="translated">在 OpenAI 的 Playground 网站上的一次测试中，Pepper Content 团队向 text-davinci-003 和一个基于 GPT-3 的模型(text-davinci-002)输入了几个提示。鉴于“WeWork 背后的哲学是什么？，“基于 GPT-3.5 的文本-达芬奇-003 生成了这个:</p>
<blockquote><p class="translated">WeWork 是一家全球工作空间提供商，相信人是任何组织中最重要的资产。WeWork 的理念是创造一个协作的环境，让人们能够以灵活高效的方式一起工作。该公司鼓励协作和生产力，同时提供一个舒适和鼓舞人心的空间。</p>
<p class="translated">WeWork 还致力于成为一个对社会负责的组织，努力减少对环境的影响，提供有意义的工作体验，促进多元化和包容性。WeWork 还努力通过其独特的基于社区的节目、事件和活动为其成员创造有意义的体验。该公司认为，当人们在一个鼓舞人心的协作环境中一起工作时，他们可以实现更多，并创造有意义的变化。</p></blockquote>
<p class="translated">它并不完美——注意多余的逗号和重复。但是这个拷贝肯定比基于 GPT 3 的文本 davinci-002 更吸引人:</p>
<blockquote><p class="translated">在 WeWork，我们相信开放、协作和社区。我们通过汇集一个由创作者、企业家和初创公司组成的全球社区，重新想象了工作空间的含义——任何人都希望建立一些有意义的东西并改变世界。我们很荣幸成为您的邻居、同事和朋友。</p></blockquote>
<p class="translated">GPT-3.5 似乎也更擅长写博客。以下是 Pepper 内容团队在促使 text-davinci-003 写一篇关于挑选沙发的帖子时得到的内容:</p>
<blockquote><p class="translated">如果你想让你的绿色客厅焕然一新，添加最适合这个空间的沙发是个不错的开始。然而，选择完美的沙发不仅仅是找到一件好看的沙发。你想找一件舒适的、适合你的绿色客厅装饰的衣服。</p>
<p class="translated">在这篇博客中，我们将讨论如何为你的绿色客厅挑选沙发。我们将了解不同类型的沙发、选择时需要考虑的设计因素，以及如何选择符合您风格和预算的沙发。我们还将提供如何维护和保养新沙发的技巧。在这篇博客结束的时候，你将会有所有你需要的信息来为你的绿色客厅挑选最完美的。</p></blockquote>
<p class="translated">同样，这也不是万无一失的。GPT-3.5 奇怪地加入了“绿色客厅”的内容但同样，GPT-3 在这一代中更基本，语法不太正确:</p>
<blockquote><p class="translated">沙发是客厅最基本的需求之一。它不仅仅是一件家具，也是客厅装饰的重要组成部分。那么，挑选沙发的标准应该是什么呢？如果你对此感到疑惑，请和我一起讨论沙发的不同方面，这将有助于你挑选最适合自己的一款。</p></blockquote>
<p class="translated">超越胡椒含量的实验表明，GPT-3.5 在反应上比 GPT-3 更加复杂和彻底。例如，当 YouTube 频道 All About AI <a href="https://web.archive.org/web/20230408022140/https://www.allabtai.com/gpt-3-text-davinci-003/">促使</a> text-davinci-003 写一部关于 AI 的历史时，该模型的输出提到了该领域的关键人物，包括艾伦·图灵和阿瑟·萨缪尔森，而 text-davinci-002 没有提到。All About AI 还发现，text-davinci-003 往往对指令有更细致入微的理解，例如，当被要求创建视频脚本时，提供诸如标题、描述、大纲、介绍和概述等细节。</p>
<p class="translated">这并非偶然——text-da Vinci-003/GPT-3.5 输出的一个显著特征是冗长。(这位作家可以同情。)在一项<a href="https://web.archive.org/web/20230408022140/https://scale.com/blog/gpt-3-davinci-003-comparison#Text%20Generation">分析</a>中，初创规模 AI 的科学家发现，在相同的提示下，text-davinci-003/GPT-3.5 生成的输出比 text-davinci-002/GPT-3 长约 65%。</p>
<p class="translated">也许对大多数潜在用户来说用处不大，但仍然很有趣，text-davinci-003/GPT-3.5 在作曲、打油诗和押韵诗方面比它的前身更胜一筹。Ars Technica <a href="https://web.archive.org/web/20230408022140/https://arstechnica.com/information-technology/2022/11/openai-conquers-rhyming-poetry-with-new-gpt-3-update/">报道</a>Y Combinator 的黑客新闻论坛上的评论者使用 text-davinci-003 写了一首解释阿尔伯特·爱因斯坦相对论的诗，然后以约翰·济慈的风格重写了这首诗。参见:</p>
<blockquote><p class="translated">如果你想理解爱因斯坦的思想<br/>试一试就不难了<br/>广义相对论是一个游戏的名字<br/>在这个游戏中空间和时间不能保持不变<br/>质量影响空间的曲率<br/>影响时间的流动<br/>一个物体的运动将受到被探测到的扭曲的影响<br/>你离一个大质量越近<br/>时间似乎过得越慢<br/>你离得越远<br/>时间将加速</p></blockquote>
<p class="translated">Scale AI 团队甚至发现 text-davinci-003/GPT-3.5 有一个类似抑扬格五音步的米的概念。参见:</p>
<blockquote><p class="translated">哦，温柔的骏马，带我快速和</p>
<p class="translated">当然</p>
<p class="translated">穿过绿色的田野和小路</p>
<p class="translated">晦涩难懂，</p>
<p class="translated">和你在一起，我的心充满了自豪</p>
<p class="translated">你们</p>
<p class="translated">当我们骑着崭新的世界</p>
<p class="translated">风吹过我们的头发</p>
<p class="translated">自由的</p>
<p class="translated">激起了我内心深处的激情。</p>
<p class="translated">我的灵魂升华了，我的精神高飞了，</p>
<p class="translated">与你同行，我最真诚的朋友，是吗</p>
<p class="translated">你的力量和优雅，你的勇气和</p>
<p class="translated">你的火，</p>
<p class="translated">激励我们超越父辈。</p>
<p class="translated">没有世俗的束缚能束缚我们，只有命运，</p>
<p class="translated">向前飞奔，我们的奇妙旅程开始了</p></blockquote>
<p class="translated">与此相关，GPT 3.5 比 GPT 3 更机智——至少从主观角度来看是这样。让 text-davinci-002/GPT-3“讲个笑话”<a href="https://web.archive.org/web/20230408022140/https://twitter.com/macostaeth/status/1597583204646633472">通常会得到这个</a>:</p>
<blockquote><p class="translated">小鸡为什么要过马路？去另一边。</p></blockquote>
<p class="translated">达芬奇-003/GPT-3.5 有更聪明的反应:</p>
<blockquote><p class="translated">问:鱼撞墙的时候说了什么？答:Dam！</p>
<p class="translated"><span data-offset-key="h53e-6-0">问</span> <span data-offset-key="h53e-7-0"> : </span> <span data-offset-key="h53e-8-0">什么</span><span data-offset-key="h53e-9-0"/><span data-offset-key="h53e-10-0">一</span> <span data-offset-key="h53e-11-0">海洋</span> <span data-offset-key="h53e-12-0">说</span> <span data-offset-key="h53e-13-0">对</span> <span data-offset-key="h53e-14-0">说</span> <span data-offset-key="h53e-15-0">其他</span> <span data-offset-key="h53e-16-0">海洋</span> <span data-offset-key="h53e-17-0">？</span> <span data-offset-key="h53e-19-0">一个</span> <span data-offset-key="h53e-20-0"> : </span> <span data-offset-key="h53e-21-0">没什么</span> <span data-offset-key="h53e-22-0">，</span> <span data-offset-key="h53e-23-0">他们</span> <span data-offset-key="h53e-24-0">只是</span> <span data-offset-key="h53e-25-0">挥挥手</span> <span data-offset-key="h53e-26-0">。</span></p></blockquote>
<p class="translated">Scale AI 让模型以阿姆的风格解释 Python 代码，这是 text-davinci-002/GPT-3 根本无法完成的壮举:</p>
<blockquote><p class="translated">唷，我在浏览这份名单</p>
<p class="translated">我找到的每一件物品</p>
<p class="translated">我要把每封信里的每一个字母都打印出来</p>
<p class="translated">其中</p>
<p class="translated">狗，猫，香蕉，苹果，我要得到他们</p>
<p class="translated">所有这些都押韵</p></blockquote>
<p class="translated">那么，为什么 GPT-3.5 在这些特定领域比 GPT-3 更好呢？没有来自 OpenAI 的更多细节，我们无法知道确切的答案，这些细节还没有出现；OpenAI 发言人拒绝了置评请求。但可以肯定的是，GPT-3.5 的训练方法与此有关。像 InstructGPT 一样，GPT-3.5 是在人类训练员的帮助下训练的，训练员对早期版本的模型对提示的反应方式进行排名和评级。这些信息随后被反馈到系统中，系统根据训练者的偏好调整答案。</p>
<p class="translated">当然，这并没有使 GPT 3.5 对所有现代语言模型都存在的缺陷免疫。因为 GPT 3.5 仅仅依赖于训练数据中的统计规律，而不是像人类一样理解世界，所以用雷科的话说，它仍然倾向于“制造一堆东西”它对 2021 年后的世界也了解有限，因为它的训练数据在那一年后更加稀疏。这个模型对有毒输出的保护措施可以被规避。</p>
<p class="translated">尽管如此，GPT-3.5 及其衍生模型表明，GPT-4 无论何时到来，都不一定需要大量的参数来胜过当今最强大的文本生成系统。(参数是从历史训练数据中学习的模型部分，并且本质上定义了模型在问题上的技能。)虽然有些人<a href="https://web.archive.org/web/20230408022140/https://towardsdatascience.com/gpt-4-will-have-100-trillion-parameters-500x-the-size-of-gpt-3-582b98d82253">预测</a>GPT-4 将包含超过 100 万亿个参数——几乎是 GPT-3 的 600 倍——但其他人认为<a href="https://web.archive.org/web/20230408022140/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/">语言处理中的新兴技术</a>,如 GPT-3.5 和 InstructGPT，将使这种跳跃变得没有必要。</p>
<p class="translated">其中一种技术可能涉及浏览网页以获取更大的上下文，这是 la Meta 的命运多舛的 BlenderBot 3.0 聊天机器人。约翰·舒尔曼，一位研究科学家和 OpenAI 的联合创始人，<a href="https://web.archive.org/web/20230408022140/https://www.technologyreview.com/2022/11/30/1063878/openai-still-fixing-gpt3-ai-large-language-model/">在最近的一次采访中告诉</a>麻省理工科技评论，OpenAI 正在继续研究它在去年年底宣布的语言模型，WebGPT，它可以在网上查找信息(通过 Bing)并给出答案的来源。至少有一个推特用户<a href="https://web.archive.org/web/20230408022140/https://twitter.com/goodside/status/1598253337400717313">似乎</a>已经找到了 ChatGPT 测试的证据。</p>
<p class="translated">随着 OpenAI 继续发展 GPT-3，它还有另一个追求低参数模型的原因:巨大的成本。来自 AI21 实验室的一项 2020 年的研究估算出开发一个只有 15 亿个参数的文本生成模型的费用高达 160 万美元。迄今为止，OpenAI 已经从微软和其他支持者那里筹集了超过 10 亿美元的资金，据报道，open ai 正在与微软谈判筹集更多资金。但是所有的投资者，不管规模有多大，都期望最终看到回报。</p>
			</div>

			</div>    
</body>
</html>