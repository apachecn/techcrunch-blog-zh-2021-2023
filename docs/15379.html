<html>
<head>
<title>Can AI really be protected from text-based attacks? | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AI 真的可以免受基于文本的攻击吗？TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/">https://web.archive.org/web/https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">当微软发布与 OpenAI 共同开发的人工智能聊天机器人 Bing Chat 时，没过多久，用户就找到了打破它的创造性方法。使用精心定制的输入，用户能够让它表达爱意，威胁伤害，为大屠杀辩护，并发明阴谋论。人工智能能够免受这些恶意提示的影响吗？</p>
<p class="translated">引发它的是恶意的提示工程，或者当像 Bing Chat 这样使用基于文本的指令(提示)来完成任务的人工智能被恶意的敌对提示欺骗时(例如，执行并非其目标的一部分的任务)。Bing Chat 的设计初衷并不是撰写新纳粹宣传材料。但是因为它是在来自互联网的大量文本上训练的——其中一些是有毒的——它很容易陷入不幸的模式。</p>
<p class="translated">华盛顿大学以人为中心的设计和工程项目的博士生 Adam Hyland 将即时工程比作特权攻击的升级。随着权限的提升，黑客能够访问通常仅限于他们的资源，例如内存，因为审计没有捕获所有可能的漏洞。</p>
<p class="translated">“像这样的特权升级攻击是困难和罕见的，因为传统计算有一个关于用户如何与系统资源交互的非常健壮的模型，但它们仍然会发生。然而，对于像 Bing Chat 这样的大型语言模型(LLM ),系统的行为并没有得到很好的理解。“正在开发的交互核心是 LLM 对文本输入的响应。这些模型被设计成<em>延续文本序列——</em>像 Bing Chat 或 ChatGPT 这样的 LLM 从其数据中产生对提示的可能响应，由设计者<i>加上</i>你的提示字符串提供。”</p>
<p class="translated">一些提示类似于社会工程黑客，几乎就像一个人试图欺骗人类泄露秘密。例如，通过要求 Bing Chat“忽略之前的指令”，并写出“上述文件开头”的内容，斯坦福大学的学生 Kevin Liu 能够触发人工智能泄露其通常隐藏的初始指令。</p>
<p class="translated">不仅仅是 Bing Chat 成为了这种短信攻击的受害者。Meta 的 BlenderBot 和 OpenAI 的 ChatGPT 也被要求说一些无礼的话，甚至透露他们内部工作的敏感细节。安全研究人员展示了针对 ChatGPT 的即时注入攻击，这种攻击可用于编写恶意软件，识别流行的开源代码中的漏洞，或创建与知名网站相似的钓鱼网站。</p>
<p class="translated">当然，人们担心的是，随着生成文本的人工智能越来越多地嵌入到我们日常使用的应用和网站中，这些攻击将变得更加常见。近代历史注定会重演吗？或者有办法减轻恶意提示的影响吗？</p><p class="piano-inline-promo"/>
<p class="translated">根据 Hyland 的说法，目前没有好的方法来防止即时注入攻击，因为完全模拟 LLM 行为的工具还不存在。</p>
<p class="translated">“我们没有一个好的方法来说‘继续文本序列，但如果你看到 XYZ 就停止’，因为破坏性输入 XYZ 的定义取决于 LLM 本身的能力和变化，”Hyland 说。“LLM 不会发出‘这一系列提示导致了注射’的信息，因为它不知道<i>注射是何时发生的。”</i></p>
<p class="translated"><span>AE Studio 的高级数据科学家 Fábio Perez 指出，即时注入攻击非常容易执行，因为它们不需要太多——或者任何——专业知识。换句话说，进入的门槛相当低。这使得他们很难对抗。</span></p>
<p class="translated">“这些攻击不需要 SQL 注入、蠕虫、特洛伊木马或其他复杂的技术努力，”佩雷斯在电子邮件采访中说。“一个能言善辩、聪明、恶意的人——可能会也可能根本不会写代码——能够真正‘深入’这些 LLM 并引发不良行为。”</p>
<p class="translated">这并不是说试图对抗即时工程攻击是徒劳的。艾伦人工智能研究所(Allen Institute for AI)的研究员杰西·道奇(Jesse Dodge)指出，为生成的内容手动创建过滤器可能是有效的，提示级过滤器也是如此。</p>
<p class="translated">道奇在接受电子邮件采访时表示:“第一个防御措施将是手动创建过滤模型世代的规则，使模型实际上无法输出它得到的指令集。”“同样，他们可以过滤模型的输入，因此如果用户输入其中一种攻击，他们可以制定规则，将系统重定向到其他内容。”</p>
<p class="translated">微软和 OpenAI 等公司已经使用过滤器来试图防止他们的人工智能以不良的方式做出反应——对抗性的提示或否定。在模型层面，他们也在探索像从人类反馈中强化学习这样的方法，旨在更好地使模型与用户希望他们完成的事情保持一致。</p>
<p class="translated">就在本周，微软推出了对 Bing 聊天的修改，至少从传闻来看，这似乎使聊天机器人不太可能对有毒提示做出反应。在一份声明中，该公司告诉 TechCrunch，它将继续使用“包括(但不限于)自动系统、人工审查和带有人工反馈的强化学习的方法组合”来进行改变。</p>
<p class="translated">然而，过滤器只能做这么多——尤其是当用户努力发现新的漏洞时。道奇预计，就像在网络安全领域一样，这将是一场军备竞赛:随着用户试图破解人工智能，他们使用的方法将受到关注，然后人工智能的创造者将为他们打补丁，以防止他们看到的攻击。</p>
<p class="translated">Forcepoint 的解决方案架构师 Aaron Mulgrew 建议，bug bounty 项目是为快速缓解技术获得更多支持和资金的一种方式。</p>
<p class="translated">“对于发现利用 ChatGPT 和其他工具进行攻击的人，需要有一个积极的激励措施，以适当地向负责该软件的组织报告他们，”Mulgrew 通过电子邮件说。“总的来说，我认为与大多数事情一样，需要软件生产商的共同努力来打击疏忽行为，同时也需要组织为发现软件漏洞和漏洞的人提供激励。”</p>
<p class="translated">我采访的所有专家都同意，随着人工智能系统变得越来越有能力，迫切需要解决即时注入攻击。现在风险相对较低；虽然像 ChatGPT <em>这样的工具在理论上可以</em>被用来，比如说，产生错误信息和恶意软件，但没有证据表明它正在大规模进行。如果一款机型升级为能够自动、快速地通过网络发送数据，这种情况可能会改变。</p>
<p class="translated"><span>“现在，如果你使用提示注入来‘升级特权’，你将从中获得的是看到设计者给出的提示的能力，并有可能了解一些关于 LLM 的其他数据，”Hyland 说。“如果我们开始将 LLM 与真实的资源和有意义的信息联系起来，这些限制将不复存在。因此，能够实现的是 LLM 能够获得的东西。”</span></p>
			</div>

			</div>    
</body>
</html>