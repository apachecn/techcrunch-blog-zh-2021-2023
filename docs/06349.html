<html>
<head>
<title>Google's new 'multisearch' features hint toward an AR glasses future • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌新的“多重搜索”功能暗示了增强现实眼镜的未来</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/05/11/googles-new-multisearch-features-hint-towards-an-ar-glasses-future/">https://web.archive.org/web/https://techcrunch.com/2022/05/11/googles-new-multisearch-features-hint-towards-an-ar-glasses-future/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">四月，谷歌推出了一项新的“多重搜索”功能，提供了一种同时使用文本和图像搜索网页的方式。今天，在谷歌的 I/O 开发者大会上，该公司宣布扩展这一功能，名为“我附近的多重搜索”这项功能将于 2022 年晚些时候推出，它将允许谷歌应用程序用户将图片或截图与“我附近”的文字结合起来，引导到当地零售商或餐馆的选项，那里有你正在搜索的服装、家居商品或食品。它还宣布了一项即将到来的多搜索开发，似乎是在考虑 AR 眼镜的情况下建立的，因为它可以根据你当前通过智能手机相机的取景器“看到”的内容，在一个场景中的多个对象中进行视觉搜索。</p>
<p class="translated">通过新的“附近”多重搜索查询，您将能够找到与您当前的视觉和基于文本的搜索组合相关的本地选项。例如，如果你正在做一个 DIY 项目，遇到了一个需要更换的部件，你可以用手机的摄像头拍下该部件的照片来识别它，然后找到当地的五金店，那里有替换的库存。</p>
<p class="translated">谷歌解释说，这与多重搜索的工作方式并没有太大的不同——它只是增加了本地组件。</p>
<p/><div id="attachment_2314683" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2314683" decoding="async" loading="lazy" class="wp-image-2314683 size-full" src="../Images/14eb910db36d367424647bf9f840877c.png" alt="" srcset="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-multisearch-near-me.jpg 392w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-multisearch-near-me.jpg?resize=150,142 150w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-multisearch-near-me.jpg?resize=300,284 300w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-multisearch-near-me.jpg?resize=50,47 50w" sizes="(max-width: 392px) 100vw, 392px" data-original-src="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-multisearch-near-me.jpg"/><p id="caption-attachment-2314683" class="wp-caption-text translated"><strong>图片来源:</strong>谷歌</p></div>
<p class="translated">最初，multisearch 的想法是允许用户对他们面前的一个物体提出问题，并通过颜色、品牌或其他视觉属性来提炼这些结果。如今，这项功能最适合购物搜索，因为它允许用户缩小产品搜索范围，这是标准的基于文本的网络搜索有时难以做到的。例如，用户可以拍摄一双运动鞋的照片，然后添加文字要求查看蓝色的运动鞋，以便只显示指定颜色的运动鞋。他们也可以选择访问运动鞋网站并立即购买。包括“附近”选项的扩展现在简单地进一步限制了结果，以便将用户指向提供给定产品的本地零售商。</p>
<p class="translated">在帮助用户找到当地餐馆方面，该功能的工作方式类似。在这种情况下，用户可以根据他们在美食博客或网络上其他地方找到的照片进行搜索，以了解这道菜是什么，以及哪些当地餐馆的菜单上可能有在餐厅用餐、外卖或送货的选项。在这里，谷歌搜索将图像与你在搜索附近餐馆的意图相结合，并将扫描谷歌地图上数百万张图像、评论和社区贡献来找到当地景点。</p>
<p class="translated">谷歌表示，新的“我附近”功能将在全球范围内以英语提供，并将随着时间的推移推广到更多语言。</p><p class="piano-inline-promo"/>
<p class="translated">multisearch 更有趣的新增功能是在场景中搜索。谷歌表示，在未来，用户将能够摇动他们的相机，在更广阔的场景中了解多个物体。</p>
<p class="translated">谷歌表示，这项功能可以用来扫描书店的货架，然后看到覆盖在你面前的有用的几个见解。</p>
<p/><div id="attachment_2316776" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2316776" decoding="async" loading="lazy" class="wp-image-2316776 size-full" src="../Images/857b108ef88966d25653a9af701c449d.png" alt="" srcset="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg 2678w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=150,82 150w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=300,165 300w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=768,422 768w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=680,374 680w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=1536,844 1536w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=2048,1126 2048w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=1200,660 1200w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg?resize=50,27 50w" sizes="(max-width: 2678px) 100vw, 2678px" data-original-src="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.23.07-PM.jpg"/><p id="caption-attachment-2316776" class="wp-caption-text translated"><strong>图片来源:</strong>谷歌</p></div>
<p class="translated">“为了实现这一点，我们不仅将计算机视觉、自然语言理解结合在一起，还将这些与网络和设备技术知识结合在一起，”谷歌搜索高级总监 Nick Bell 指出。“因此，这种可能性和能力将是巨大而重要的，”他指出。</p>
<p class="translated">该公司在谷歌眼镜发布之初就进入了增强现实市场，但没有证实它正在开发任何一种新的增强现实眼镜类型的设备，但暗示了这种可能性。</p>
<p class="translated">“现在有了人工智能系统，今天可能发生的事情——以及未来几年可能发生的事情——就好像打开了许多机会，”贝尔说。他指出，除了语音搜索、桌面和移动搜索，该公司认为视觉搜索也将成为未来更大的一部分。</p>
<p class="translated"><img decoding="async" loading="lazy" class="wp-image-2316771 size-full" src="../Images/4f0c0ca6092469044f15830e43164a26.png" alt="" srcset="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg 2744w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=150,79 150w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=300,159 300w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=768,407 768w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=680,360 680w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=1536,814 1536w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=2048,1085 2048w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=1200,636 1200w, https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg?resize=50,26 50w" sizes="(max-width: 2744px) 100vw, 2744px" data-original-src="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-11-at-1.22.32-PM.jpg"/></p>
<p class="translated">“现在每个月谷歌上有 80 亿次使用 Lens 的视觉搜索，这个数字是一年前的三倍，”贝尔继续说道。“我们确实从人们身上看到了视觉搜索的欲望和渴望。我们现在努力做的是深入了解用例，找出最有用的地方，”他说。“我认为，当我们思考搜索的未来时，视觉搜索肯定是其中的一个关键部分。”</p>
<p class="translated">当然，据报道，该公司正在进行一个代号为 Iris 项目的秘密项目，以制造一种新的 AR 耳机，预计将于 2024 年发布。很容易想象这种场景扫描功能如何在这样的设备上运行，以及任何类型的图像加文本(或声音！)搜索功能可以在 AR 头戴式耳机上使用。想象一下，再看一双你喜欢的运动鞋，然后让一个设备导航到你可以购买的最近的商店。</p>
<p class="translated">谷歌搜索 SVP 公司的 Prabhakar Raghavan 在谷歌 I/O 的演讲台上表示:“展望未来，这项技术可以用于解决日常需求之外的社会挑战，例如支持自然资源保护者识别需要保护的植物物种，或者帮助救灾人员在需要的时候快速整理捐赠。”</p>
<p class="translated">不幸的是，谷歌没有提供预计何时将场景扫描功能交付给用户的时间表，因为该功能仍在“开发中”</p>
<p class="translated">【T2<img decoding="async" src="../Images/b9bf38edae6492a8cf5d1c99ef2eab5a.png" alt="&quot;Read" data-original-src="https://web.archive.org/web/20230117055440im_/https://techcrunch.com/wp-content/uploads/2022/05/google-io-2022-banner.jpg"/></p>
			</div>

			</div>    
</body>
</html>