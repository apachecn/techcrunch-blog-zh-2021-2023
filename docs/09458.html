<html>
<head>
<title>Deepfakes: Uncensored AI art model prompts ethics questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Deepfakes:未经审查的人工智能艺术模型引发伦理问题</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/08/24/deepfakes-for-all-uncensored-ai-art-model-prompts-ethics-questions/">https://web.archive.org/web/https://techcrunch.com/2022/08/24/deepfakes-for-all-uncensored-ai-art-model-prompts-ethics-questions/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">一款新的开源人工智能图像生成器能够从任何文本提示中生成逼真的图片，在第一周就被迅速接受。Stability AI 的<a href="https://web.archive.org/web/20230408161258/https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/">stability Diffusion</a>，高保真，但能够在现成的消费硬件上运行，现在正被 Artbreeder、Pixelz.ai 等艺术生成器服务使用。但是这个模型未经过滤的特性意味着并非所有的使用都是光明正大的。</p>
<p class="translated">在很大程度上，用例是光明正大的。例如，NovelAI 一直在尝试稳定的扩散，以产生可以伴随用户在其平台上创建的人工智能生成的故事的艺术。<a href="https://web.archive.org/web/20230408161258/https://techcrunch.com/tag/midjourney/">midway</a>推出了一个测试版，利用稳定扩散来获得更好的真实感。</p>
<p class="translated">但是稳定扩散也被用于不太可口的目的。在臭名昭著的讨论板 4chan 上，这个模型很早就泄露了，几个线程致力于人工智能生成的裸体名人艺术和其他形式的生成色情。</p>
<p class="translated">Stability AI 的首席执行官艾玛德·莫斯塔克(Andrew Mostaque)称该模型在 4chan 上泄露是“不幸的”，并强调该公司正在与“领先的伦理学家和技术”就安全和其他负责任的发布机制进行合作。其中一个机制是一个可调整的人工智能工具，安全分类器，包含在整体稳定扩散软件包中，试图检测和阻止攻击性或不良图像。</p>
<p class="translated">但是，安全分类器(默认情况下打开)可以禁用。</p>
<p class="translated">稳定扩散是一个全新的领域。其他人工智能艺术生成系统，如 OpenAI 的 DALL-E 2，对色情材料实施了严格的过滤。(开源稳定扩散的<a href="https://web.archive.org/web/20230408161258/https://huggingface.co/spaces/CompVis/stable-diffusion-license" target="_blank" rel="noopener">许可</a>禁止某些应用，比如剥削未成年人，但是模型本身在技术层面上没有束缚。)而且，很多不具备公众人物艺术创作能力，不像稳定扩散。这两种能力结合起来可能会有风险，让坏演员制造色情“深度假货”——最糟糕的情况是——可能会延续虐待或牵连某人犯下他们没有犯下的罪行。</p>
<p class="translated">不幸的是，女性最有可能成为这种情况的受害者。<a href="https://web.archive.org/web/20230408161258/https://www.wired.com/story/most-deepfakes-porn-multiplying-fast/" target="_blank" rel="noopener">2019 年进行的一项研究</a>显示，在 90%至 95%未经双方同意的假货中，约 90%是女性。Mission Control 负责人工智能的副总裁 Ravit Dotan 表示，这对这些人工智能系统的未来来说不是一个好兆头。</p><p class="piano-inline-promo"/>
<p class="translated">“我担心非法内容合成图像的其他影响——这将加剧所描绘的非法行为，”多坦通过电子邮件告诉 TechCrunch。“例如，合成儿童[剥削]会增加真实儿童[剥削]的创造吗？会不会增加恋童癖发作的次数？"</p>
<p class="translated">蒙特利尔人工智能伦理研究所首席研究员 Abhishek Gupta 同意这一观点。“我们真的需要考虑人工智能系统的生命周期，包括部署后的使用和监控，并考虑我们如何才能设想即使在最糟糕的情况下也能最大限度地减少伤害的控制措施，”他说。“当一种强大的能力(如稳定的扩散)泛滥时，这种情况尤其如此，这种能力可能会对这种系统可能针对的人造成真正的创伤，例如，以受害者的形象制作令人反感的内容。”</p>
<p class="translated">在过去的一年里，一个类似于<a href="https://web.archive.org/web/20230408161258/https://www.nytimes.com/2022/08/21/technology/google-surveillance-toddler-photo.html" target="_blank" rel="noopener">的预演</a>上演了，在一名护士的建议下，一名父亲拍下了他年幼的孩子肿胀的生殖器部位的照片，并用短信发送到护士的 iPhone 上。这张照片自动备份到谷歌照片，并被该公司的人工智能过滤器标记为儿童性虐待材料，这导致该男子的账户被禁用，并受到旧金山警察局的调查。</p>
<p class="translated">像 Dotan 这样的专家说，如果一张合法的照片可以通过这样的检测系统，那么像 Stable Diffusion 这样的系统产生的 deepfakes 没有理由不能——而且是大规模的。</p>
<p class="translated">“人们创造的人工智能系统，即使他们有最好的意图，也可能被用于他们没有预料到也无法阻止的有害方式，”多坦说。"我认为开发者和研究人员经常低估这一点."</p>
<p class="translated">当然，创造 deepfakes 的技术已经存在了一段时间，无论是人工智能驱动还是其他方式。deepfake 检测公司<a href="https://web.archive.org/web/20230408161258/https://sensity.ai/reports/" target="_blank" rel="noopener">Sensity</a>2020 年的一份报告发现，每月有数百个以女明星为主角的露骨 deepfake 视频被上传到世界上最大的色情网站；该报告估计，网上 deepfakes 的总数约为 49，000 个，其中超过 95%是色情内容。<span>自从人工智能人脸交换工具几年前进入主流以来，包括艾玛·沃森、娜塔莉·波特曼、比莉·艾莉丝和泰勒·斯威夫特在内的女演员一直是 deepfakes 的目标，包括基丝汀·贝尔在内的一些女演员公开反对他们认为的<a href="https://web.archive.org/web/20230408161258/https://www.youtube.com/watch?time_continue=2&amp;v=hHHCrf2-x6w" target="_blank" rel="noopener">性剥削</a>。</span></p>
<p class="translated">但是稳定扩散代表了新一代的系统，可以通过用户最少的工作创建令人难以置信的——如果不是完美的——令人信服的假图像。它也很容易安装，只需要几个安装文件和一个几百美元的高端显卡。可以在 M1 MacBook 上运行的更高效的系统版本的工作正在进行中。</p>
<p class="translated">伦敦玛丽女王大学人工智能小组的博士研究员塞巴斯蒂安·伯恩斯认为，自动化和扩大定制图像生成的可能性是与稳定扩散等系统的巨大差异——也是主要问题。“大多数有害的图像已经可以用传统的方法制作出来，但是是手工的，需要很多努力，”他说。"一个可以产生近乎照片般真实的镜头的模型可能会让位于对个人的个性化勒索攻击."</p>
<p class="translated">伯恩斯担心，从社交媒体上搜集的个人照片可能被用来调节稳定传播或任何此类模式，以生成有针对性的色情图像或描述非法行为的图像。这肯定有先例。在 2018 年报道了一名 8 岁的克什米尔女孩被强奸后，印度调查记者 Rana Ayyub <a href="https://web.archive.org/web/20230408161258/https://www.huffpost.com/archive/in/entry/deepfake-porn_a_23595592" target="_blank" rel="noopener">成为印度民族主义巨魔的目标</a>，其中一些人制作了她的脸在另一个人身上的 deepfake 色情作品。民族主义政党 BJP 的领导人也分享了这一消息，Ayyub 因此受到的骚扰变得如此严重，联合国不得不进行干预。</p>
<p class="translated">“稳定的传播提供了足够的定制化，可以自动发出针对个人的威胁，要么付费，要么冒着伪造但具有潜在破坏性的视频被发布的风险，”伯恩斯继续说道。“我们已经看到有人在网络摄像头被远程访问后遭到勒索。渗透步骤可能不再必要了。”</p>
<p class="translated">随着在野外的稳定传播，并且已经被用来制作色情作品——有些是非自愿的——图像主持人可能有责任采取行动。TechCrunch 联系了主要的成人内容平台之一 OnlyFans，后者表示将“持续”更新其技术，以“应对创作者和粉丝安全的最新威胁，包括 deepfakes。”</p>
<p class="translated">“OnlyFans 上的所有内容都经过最先进的数字技术审查，然后由我们训练有素的人类版主进行人工审查，以确保内容中的任何人都是经过验证的 OnlyFans 创作者，或者我们有有效的发布表格，”OnlyFans 发言人通过电子邮件表示。"任何我们怀疑可能是深度伪造的内容都会被停用."</p>
<p class="translated">Patreon 也允许成人内容，其发言人指出，该公司有一项反对 deepfakes 的政策，不允许“重新利用名人肖像并将非成人内容置于成人背景下”的图像。"</p>
<p class="translated">“Patreon 不断监控新出现的风险，比如[人工智能生成的 deepfakes]。如今，我们确实制定了政策，不允许对真实的人实施虐待行为，也不允许任何可能对现实世界造成伤害的事情，”帕特里翁的发言人在一封电子邮件中继续说道。“随着技术或新的潜在风险的出现，我们将遵循我们现有的流程:与创作者密切合作，为 Patreon 制定政策，包括允许哪些好处以及什么样的内容符合指导原则。”</p>

<p class="translated">然而，如果历史可以借鉴的话，<a href="https://web.archive.org/web/20230408161258/https://www.wired.co.uk/article/deepfake-porn-websites-videos-law" target="_blank" rel="noopener">执法</a>可能会不均衡——部分是因为很少有法律专门保护深度造假，因为它与色情有关。即使法律行动的威胁将一些致力于令人反感的人工智能内容的网站拉下，也没有什么可以阻止新的网站出现。</p>
<p class="translated">换句话说，古普塔说，这是一个勇敢的新世界。</p>
<p class="translated">“有创意和恶意的用户可以滥用(稳定扩散)的能力，大规模产生主观上令人反感的内容，使用最少的资源进行推理——这比训练整个模型更便宜——然后在 4chan 这样的网站上发布它们，以增加流量和吸引注意力，”古普塔说。“当这种能力‘消失’时，诸如 API 速率限制、对从系统返回的各种输出的安全控制等控制措施不再适用，就会有很大的风险。”</p>
<p class="translated"><em>编者按:这篇文章的早期版本包含了一些名人造假的图片，但这些图片已经被删除了。</em></p>
			</div>

			</div>    
</body>
</html>