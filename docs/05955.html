<html>
<head>
<title>The emerging types of language models and why they matter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型的新兴类型及其重要性</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/">https://web.archive.org/web/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">理解和生成文本的人工智能系统被称为语言模型，是企业中炙手可热的新事物。最近<a href="https://web.archive.org/web/20230405062114/https://www.johnsnowlabs.com/natural-language-processing-named-a-foundational-ai-technology-according-to-the-2021-ai-in-healthcare-survey-report/">的一项调查</a>发现，60%的科技领导者表示，他们在人工智能语言技术上的预算在 2020 年至少增加了 10%，而 33%的人称增加了 30%。</p>
<p class="translated">但是并不是所有的语言模型都是平等的。几种类型正在成为主导，包括大型通用模型，如 OpenAI 的 GPT-3，以及针对特定任务进行微调的模型(想想回答 IT 桌面问题)。在边缘存在第三种类型的模型——这种模型倾向于高度压缩大小，仅限于少数功能，专门设计为在物联网设备和工作站上运行。</p>
<p class="translated">这些不同的方法在优点、缺点和要求方面有很大的不同——下面是它们之间的比较，以及在接下来的一两年里，它们将在哪里部署。</p>
<h2 class="translated">大型语言模型</h2>
<p class="translated">一般来说，大型语言模型的大小为数十亿字节，并在大量文本数据上进行训练，有时达到千兆字节的规模。就参数数量而言，它们也是最大的模型之一，其中“参数”是指模型在学习时可以独立改变的值。参数是从历史训练数据中学习的模型的部分，并且本质上定义了模型在问题上的技能，例如生成文本。</p>
<p class="translated">卡耐基梅隆大学(Carnegie Mellon)自然语言处理专业的博士生徐方正(音译)通过电子邮件告诉 TechCrunch 说:“大模型用于零场景或少场景，在这些场景中，很少领域[量身定制]的训练数据可用，并且通常能够<em>好的</em>根据一些提示生成一些东西。”。在机器学习中，“少射”指的是用最少的数据训练模型的做法，而“零射”则意味着模型可以学习识别它在训练中没有明确看到的东西。</p>
<p class="translated">“一个单一的大型模型可能在很少训练数据的情况下实现许多下游任务，”徐继续说。</p>
<p class="translated">随着研究人员开发更新更大的体系结构，大型语言模型的使用在过去几年中急剧增长。2020 年 6 月，人工智能初创公司 OpenAI 发布了 GPT-3，这是一个拥有 1750 亿个参数的模型，可以在给出包含指令的简短提示的情况下生成文本甚至代码。开放研究组织 EleutherAI 随后推出了 GPT J，这是一个较小的(60 亿个参数)但功能强大的语言模型，可以在语言之间进行翻译，撰写博客文章，完成代码等等。最近，微软和英伟达开源了一个被称为威震天-图灵自然语言生成(MT-NLG)的模型，这是迄今为止以 5300 亿个参数开发的最大的阅读理解和自然语言推理<a href="https://web.archive.org/web/20230405062114/https://www.analyticsvidhya.com/blog/2021/05/bert-for-natural-language-inference-simplified-in-pytorch/">模型之一。</a></p><p class="piano-inline-promo"/>
<p class="translated"><span>“这些大型语言模型仍然如此引人注目的一个原因是，一个单一的模型可以用于各种任务”，包括</span>问题回答、文档摘要、文本生成、句子完成、翻译等等，<span>加州大学洛杉矶分校的计算社会科学家 Bernard Koch 通过电子邮件告诉 TechCrunch。"</span> <span>第二个原因是，当你向模型中添加更多参数和数据时，它们的性能会继续扩展……超大型预训练语言模型引人注目的第三个原因是，在给定少量有标签的例子时，它们似乎能够做出不错的预测。"</span></p>
<p class="translated">包括 Cohere 和 AI21 Labs 在内的初创公司也通过 API 提供类似于 GPT-3 的模型。其他公司，特别是像谷歌这样的科技巨头，已经选择将他们开发的大型语言模型藏在内部。例如，谷歌最近详细介绍了一个名为 PaLM 的 5400 亿参数模型，但拒绝发布，该公司声称该模型在跨语言任务方面实现了最先进的性能。</p>
<p class="translated">大型语言模型，无论开源与否，都有着巨大的开发成本。来自 AI21 实验室的一项 2020 年的研究估算出开发一个只有 15 亿个参数的文本生成模型的费用高达 160 万美元。推理——实际运行训练好的模型——是另一个消耗。一个消息来源<a href="https://web.archive.org/web/20230405062114/https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/">估计</a>在单个 AWS 实例(<a href="https://web.archive.org/web/20230405062114/https://aws.amazon.com/ec2/instance-types/p3/"> p3dn.24xlarge </a>)上运行 GPT-3 的成本至少为每年 87，000 美元。</p>
<p class="translated">“大型模型将变得更大，更强大，更通用，更多模态，训练成本更低。只有大型科技公司和资金极其雄厚的初创公司才能玩这个游戏，”AI2 孵化器的技术总监 Vu Ha 通过电子邮件告诉 TechCrunch。“大型模型非常适合原型制作、构建新颖的概念验证和评估技术可行性。由于成本原因，它们很少是现实部署的正确选择。如果使用 GPT-3，一个定期处理推文、闲置消息、电子邮件等的应用程序将变得成本高昂。”</p>
<p class="translated">大型语言模型将继续成为云服务和 API 的标准，其中多功能性和企业访问比延迟更重要。但是尽管最近有了<a href="https://web.archive.org/web/20230405062114/https://techcrunch.com/2022/03/22/microsoft-improves-its-ai-translations-with-z-code/"/><a href="https://web.archive.org/web/20230405062114/https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens">的创新</a>，这些类型的语言模型对大多数组织来说仍然是不切实际的，无论是学术界、公共部门还是私营部门。</p>
<h2 class="translated">微调的语言模型</h2>
<p class="translated">微调模型通常比大型语言模型要小。例子包括 OpenAI 的 Codex，它是 GPT-3 的直接后代，为编程任务进行了微调。虽然仍然包含数十亿个参数，但 Codex 比 OpenAI 更小，并且在生成和完成计算机代码字符串方面更好。</p>
<p class="translated">微调可以提高模型执行任务的能力，例如<a href="https://web.archive.org/web/20230405062114/https://techcrunch.com/2022/01/24/ai2-shows-off-an-open-qa-focused-rival-to-gpt3/">回答问题</a>或生成蛋白质序列(如 Salesforce 的<a href="https://web.archive.org/web/20230405062114/https://blog.salesforceairesearch.com/progen/"> ProGen </a>)。但它也能增强模特对某些主题的理解，比如临床研究。</p>
<p class="translated">“微调…模型适用于具有大量训练数据的成熟任务，”徐说。"例子包括机器翻译、问题回答、命名实体识别、实体链接和信息检索."</p>
<p class="translated">优势不止于此。因为微调模型是从现有的语言模型中派生出来的，所以微调模型不需要花费太多的时间(或计算)来训练或运行。(像上面提到的那些更大的模型可能需要几周时间，或者需要更多的计算能力才能在几天内训练出来。)它们也不需要像大型语言模型那样多的数据。GPT-3 是在 45tb 的文本上训练的，而 Codex 是在 159 的文本上训练的。</p>
<p class="translated">微调已经应用于许多领域，但是最近一个特别强大的例子是 OpenAI 的 InstructGPT。使用一种称为“从人类反馈中强化学习”的技术，OpenAI 收集了一组人类编写的演示数据集，这些演示是关于提交给 OpenAI API 的提示和由人类数据标签团队编写的提示的。他们利用这些数据集创建了 GPT-3 的微调分支——除了是 GPT-3 的百分之一大小之外——在密切符合用户意图的同时，明显不太可能生成<a href="https://web.archive.org/web/20230405062114/https://techcrunch.com/2020/08/07/here-are-a-few-ways-gpt-3-can-go-wrong/">有问题的文本</a>。</p>
<p class="translated">在微调力量的另一次展示中，谷歌的研究人员在 2 月份发表了一项<a href="https://web.archive.org/web/20230405062114/https://arxiv.org/pdf/2109.01652.pdf">研究</a>，声称一个远小于 GPT-3 的模型——微调语言网(FLAN)——在一些具有挑战性的基准测试中“大幅度”胜过 GPT-3。FLAN 有 1370 亿个参数，在研究人员测试的 25 项任务中，有 19 项超过了 GPT-3，甚至在 10 项任务中超过了 GPT-3。</p>
<p class="translated">“我认为微调可能是目前行业中使用最广泛的方法，我认为这在短期内不会改变。目前，对较小语言模型的微调允许用户更多地控制使用他们自己的特定领域数据来解决他们的专业问题，”Koch 说。“<span>公司不再分发(超大型语言)模型供用户自行微调，而是通过 API 提示将少量学习商业化，用户可以给模型提供简短提示和示例。”</span></p>
<h2 class="translated">边缘语言模型</h2>
<p class="translated">Edge 模型有目的地缩小尺寸，<em>可以</em>采取微调模型的形式——但并不总是这样。有时，他们在小数据集上从头开始接受训练，以满足特定的硬件限制(例如，电话或本地 web 服务器硬件)。无论如何，edge 模型——尽管在某些方面有局限性——提供了大量大型语言模型无法比拟的优势。</p>
<p class="translated">成本是一个主要因素。使用离线和设备上运行的 edge 模型，无需支付任何云使用费。(即使是微调过的模型也往往太大，无法在本地机器上运行；MT-NLG 在桌面处理器上生成文本需要一分多钟。)像分析数百万条推文这样的任务可能会在流行的基于云的模型上积累数千美元的费用。</p>
<p class="translated">理论上，边缘模式也比互联网模式提供了更大的隐私，因为它们不需要在云端传输或分析数据。它们的速度也更快，这是翻译等应用程序的一个关键优势。谷歌翻译等应用依赖边缘模型来提供离线翻译。</p>
<p class="translated">“边缘计算很可能被部署在需要即时反馈的环境中……总的来说，我认为这些场景是人类与人工智能或机器人或自动驾驶汽车阅读路标之类的东西进行对话互动的场景，”科赫说。"<span>作为一个假设的例子，Nvidia 有一个演示，其中一个 edge 聊天机器人在一家快餐店与客户进行了对话。最后一个用例可能是电子病历中的自动笔记。在这种情况下，快速处理对话至关重要。”</span></p>
<p class="translated">当然，小模型不能完成大模型能完成的所有事情。它们受到边缘设备中的硬件的限制，从单核处理器到配备 GPU 的片上系统。此外，一些研究表明，用于开发它们的技术会<a href="https://web.archive.org/web/20230405062114/https://arxiv.org/abs/2010.03058">放大不想要的特征</a>，比如算法偏差。</p>
<p class="translated">“[通常]在功耗和预测能力之间有一个权衡。此外，移动设备计算并没有真正与分布式高性能计算集群同步增长，因此性能可能会越来越落后，”徐说。</p>
<h2 class="translated">展望未来</h2>
<p class="translated">随着大型、微调和边缘语言模型随着新的研究不断发展，它们可能会在更广泛采用的道路上遇到障碍。例如，虽然与从头开始训练模型相比，微调模型需要更少的数据，但微调仍然需要<em>一个</em>数据集。根据领域的不同(例如，从很少使用的语言进行翻译)，数据可能不存在。</p>
<p class="translated">微调的缺点是仍然需要大量的数据。少量学习的缺点是，它的效果不如微调，并且数据科学家和机器学习工程师对模型的控制力较小，因为他们只通过 API 与模型进行交互，”Koch 继续说道。“<span>edge AI 的缺点是复杂的模型无法适应小型设备，因此性能严格来说不如能够适应单个桌面 GPU 的模型——更不用说分布在数万个 GPU 上的基于云的大型语言模型了。”</span></p>
<p class="translated">徐指出，所有的语言模型，无论大小，在某些重要方面仍然研究不足。她希望像可解释性和可解释性这样的领域——旨在理解模型如何以及为什么工作并将这些信息暴露给用户——在未来得到更多的关注和投资，特别是在像医学这样的“高风险”领域。</p>
<p class="translated">“来源真的是这些模型应该具有的重要的下一步，”徐说。“未来，将会有越来越多高效的微调技术……以适应对更大的模型进行整体微调的不断增加的成本。边缘模型将继续发挥重要作用，因为模型越大，就需要更多的研究和开发来提炼或压缩模型，以适应边缘设备。”</p>
			</div>

			</div>    
</body>
</html>