<html>
<head>
<title>Europe's AI Act falls far short on protecting fundamental rights, civil society groups warn | TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">公民社会团体警告说，欧洲的人工智能法案远未保护基本权利</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2021/11/30/eu-ai-act-civil-society-recommendations/">https://web.archive.org/web/https://techcrunch.com/2021/11/30/eu-ai-act-civil-society-recommendations/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">民间社会一直在仔细研究欧盟委员会关于基于风险的人工智能应用监管框架的提案的细节，该提案是由欧盟行政部门早在 4 月份提出的。</p>
<p class="translated">一百多个民间社会组织的结论是，立法草案远远没有保护基本权利免受人工智能引发的伤害，如规模歧视和黑箱偏见——他们<a href="https://web.archive.org/web/20230315095316/https://edri.org/wp-content/uploads/2021/11/Political-statement-on-AI-Act.pdf">发表了一份呼吁进行重大修改的呼吁</a>。</p>
<p class="translated">“我们特别认识到，人工智能系统加剧了权力的结构性失衡，伤害往往落在社会中最边缘化的人群身上。因此，这份集体声明列出了 11[5]个民间社会组织对一项突出基本权利的人工智能法案的呼吁，”他们写道，并在完整的建议声明中确定了九个“目标”(每个目标都有各种修改建议)。</p>
<p class="translated">起草立法的委员会<a href="https://web.archive.org/web/20230315095316/https://techcrunch.com/2020/02/19/europe-sets-out-plan-to-boost-data-reuse-and-regulate-high-risk-ais/">将人工智能法规</a>标榜为“可信的”、“以人为中心的”人工智能的框架。然而，根据民间社会团体的分析，鉴于缺乏必要的制衡措施来实际防止自动伤害，它有可能转向更接近于数据驱动的滥用的有利框架。</p>
<p class="translated">今天的声明是由欧洲数字权利组织(EDRi)、Access Now、Panoptykon 基金会、epicenter.works、AlgorithmWatch、欧洲残疾人论坛(EDF)、Bits of Freedom、Fair Trials、PICUM 和 ANEC 起草的，并得到了来自欧洲及其他地区的 115 个非营利组织的签名。</p>
<p class="translated">倡导团体希望他们的建议将被欧洲议会和理事会采纳，因为共同立法者将在最终文本在整个欧盟获得通过和应用之前，继续辩论和修改人工智能法案(AIA)提案。</p>

<p class="translated">民间社会组织的主要建议包括需要修订该法规，以采用灵活、经得起未来考验的方法来评估人工智能引发的风险——这意味着它将允许更新被认为不可接受(因此被禁止)的使用案例列表和该法规仅限制的使用案例列表，以及扩大所谓“高风险”使用(目前固定不变)列表的能力。</p>
<p class="translated">在非政府组织看来，该委员会对人工智能风险进行分类的提议过于“死板”，设计也很糟糕(该组织的声明字面上称之为“功能失调”)，无法跟上快速发展、迭代的人工智能技术以及数据驱动技术不断变化的用例的步伐。</p>
<p class="translated">“这种事先将人工智能系统指定为不同风险类别的方法没有考虑到风险水平也取决于系统部署的环境，并且无法事先完全确定，”他们写道。“此外，虽然 AIA 包括一个可以更新‘高风险’人工智能系统列表的机制，但它没有提供更新‘不可接受’(第 5 条)和有限风险(第 52 条)列表的范围。</p>
<p class="translated">「此外，虽然可更新附件三，将新系统加入高风险人工智能系统清单，但系统只能加入现有八个领域标题的范围内。这些标题目前不能在 AIA 框架内修改。该框架的这些僵化方面损害了 AIA 的持久相关性，特别是其应对未来发展和基本权利新风险的能力。”</p>
<p class="translated">他们还指责委员会在制定人工智能的禁止使用案例方面缺乏雄心——敦促“全面禁止”所有社交评分系统；公共场所的所有远程生物识别(不仅仅是对执法部门如何使用该技术的狭窄限制)；在所有的情感识别系统上；所有歧视性生物特征分类；在所有人工智能相面术上；所有用来预测未来犯罪活动的系统；以及在迁移环境中对所有系统进行描述和风险评估——主张禁止“对基本权利构成不可接受的风险的所有人工智能系统”。</p>
<p class="translated">在这一点上，这些组织的建议呼应了早些时候的呼吁，要求该法规更进一步，全面禁止远程生物特征监控——包括来自<a href="https://web.archive.org/web/20230315095316/https://techcrunch.com/2021/04/23/eus-top-data-protection-supervisor-urges-ban-on-facial-recognition-in-public/">欧盟数据保护主管</a>的呼吁。</p>

<p class="translated">民间社会团体还希望监管义务适用于高风险人工智能系统的<em>用户</em>，而不仅仅是提供商(开发者)——呼吁对用户施加强制性义务，进行并公布基本权利影响评估，以确保围绕风险的问责不能因监管主要关注提供商而规避。</p>
<p class="translated">毕竟，为一个表面目的开发的人工智能技术可能会被应用于不同的用例，从而引发不同的权利风险。</p>
<p class="translated">因此，他们希望“高风险”人工智能的用户有明确的义务公布影响评估——他们表示，这应该涵盖对人、基本权利、环境和更广泛的公共利益的潜在影响。</p>
<p class="translated">“虽然附件三所列系统构成的一些风险来自它们的设计方式，但重大风险来自它们的使用方式。这意味着提供商无法在合规性评估期间全面评估高风险人工智能系统的全部潜在影响，因此用户也必须有义务维护基本权利，”他们敦促道。</p>
<p class="translated">他们还主张将透明度要求扩大到高风险系统的用户——建议他们必须在公共数据库中注册人工智能系统的具体使用，该监管机构提议为这类系统的提供商建立该数据库。</p>
<p class="translated">“欧盟独立高风险人工智能系统数据库(第 60 条)为提高人工智能系统对受影响个人和民间社会的透明度提供了一个充满希望的机会，并可以极大地促进公共利益研究。然而，该数据库目前只包含提供商注册的高风险系统的信息，而没有使用背景的信息，”他们写道，并警告说:“这个漏洞破坏了数据库的目的，因为它将阻止公众发现高风险人工智能系统实际上在哪里、由谁以及出于什么目的使用。”</p>
<p class="translated">另一项建议针对的是民间社会对拟议框架的一项关键批评——当个人受到人工智能的负面影响时，它没有为个人提供权利和补救途径。</p>
<p class="translated">这标志着与现有欧盟数据保护法的明显背离。欧盟数据保护法赋予与个人数据相关的人一系列权利，并至少在纸面上允许他们寻求对违规行为的补救，以及第三方代表个人寻求补救。(此外,《一般数据保护条例》包括与自动处理个人数据有关的规定；第 22 条赋予受制于完全基于自动化的具有法律效力或类似效力的决定的人获得关于处理的信息的权利；和/或请求人工审查或质疑该决定。)</p>
<p class="translated">这些团体认为，受人工智能系统影响的人缺乏“有意义的权利和补偿”，这表明该框架在防范高风险自动化规模伤害的能力方面存在巨大漏洞。</p>
<p class="translated">“AIA 目前没有授予受人工智能系统影响的人个人权利，也没有包含任何个人或集体补救的规定，或人民或民间社会可以参与高风险人工智能系统调查过程的机制。因此，AIA 没有完全解决人工智能系统部署中的不透明性、复杂性、规模和权力不平衡带来的无数危害，”他们警告说。</p>
<p class="translated">他们建议对立法进行修正，纳入两项个人权利作为司法补救的基础，即:</p>
<ul>
<li class="translated">不受制于构成不可接受的风险或不符合该法的人工智能系统的权利；和</li>
<li class="translated">对于在 AIA 范围内的系统的协助下做出的决定，有权以残疾人可以理解的方式获得清晰易懂的解释；</li>
</ul>
<p class="translated">他们还建议那些“由于人工智能系统的投入使用”而权利受到侵犯的人有权获得“有效的补救”。而且，正如你可能预期的那样，民间社会组织希望建立一种机制，让像他们这样的公共利益团体能够就违反行为或与破坏基本权利或公共利益的人工智能系统有关的行为向国家监管机构提出投诉——他们指定这些行为应该引发调查。(监督机构完全无视 GDPR 的投诉是有效执行该制度的一个主要问题。)</p>
<p class="translated">这些团体声明中的其他建议包括在人工智能系统的整个生命周期中考虑可访问性的必要性，他们指出该法规中缺乏可访问性要求——警告称，这有可能导致人工智能的开发和使用“为残疾人带来更多障碍”；他们还希望有明确的限制，以确保该法规提议委托给私营标准机构的统一产品安全标准只应涵盖高风险人工智能系统的“真正技术”方面(以便政治和基本权利决定“牢牢地留在欧盟立法者的民主审查范围内”，正如他们所说的那样)；他们希望对人工智能系统用户和提供商的要求不仅适用于欧盟内部，也适用于其他地方——“以避免通过欧盟开发的技术进行歧视、监控和滥用的风险”。</p>
<p class="translated">根据这些团体的评估，可持续性和环境保护也被忽视了。</p>
<p class="translated">在这方面，他们呼吁“对人工智能系统的资源消耗和温室气体排放影响提出横向、面向公众的透明要求”——不管风险水平如何；涵盖人工智能系统设计、数据管理和培训、应用和基础设施(硬件、数据中心等)。</p>
<p class="translated">欧盟委员会经常通过吹捧自动化是一项关键技术来证明其鼓励人工智能更新的目标，这项技术使欧盟寻求在 2050 年前过渡到“气候中立”的大陆——然而，人工智能自身的能源和资源消耗是这些所谓的“智能”系统中一个被忽视的组成部分。如果没有强大的环境审计要求也适用于人工智能，声称人工智能将提供气候变化的答案只是公关。</p>
<p class="translated">已经与该委员会联系，请其对民间社会的建议作出回应。</p>
<p class="translated">上个月，欧洲议会的议员投票支持全面禁止远程生物识别监控技术，如面部识别，禁止使用私人面部识别数据库，禁止基于行为数据的预测性警务。</p>
<p class="translated">他们还投票支持禁止社交评分系统，该系统试图根据公民的行为或个性来评估公民的可信度，并支持禁止人工智能协助司法决定——这是自动化已经应用的另一个极具争议的领域。</p>
<p class="translated">因此，欧洲议会议员在致力于修改《大赦国际法》时，很可能会认真考虑公民社会的建议。</p>
<p class="translated">与此同时，该委员会正在确定其在该法规上的<a href="https://web.archive.org/web/20230315095316/https://twitter.com/mikarv/status/1465605600843341824">谈判授权</a>——当前的提案正在推动禁止私营公司的社会评分，但寻求为人工智能的研发和国家安全用途开辟道路。</p>
<p class="translated">委员会、议会和理事会之间的讨论将决定该法规的最终形式，尽管议会还必须在全体投票中批准该法规的最终文本——因此欧洲议会议员的意见将发挥关键作用。</p>

			</div>

			</div>    
</body>
</html>