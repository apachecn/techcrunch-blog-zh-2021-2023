<html>
<head>
<title>New OpenAI tool draws anything, bigger and better than ever</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新的 OpenAI 工具比以往任何时候都画得更大更好</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/">https://web.archive.org/web/https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">去年早些时候，OpenAI 展示了一个名为 DALL-E(WALL-E 和 Dali 的组合)的出色的新人工智能模型，它能够以几乎任何风格绘制几乎任何东西。但是结果很少是你想挂在墙上的东西。现在 DALL-E 2 上市了，它做的比它的前辈好得多——事实上，好得吓人。但是新的能力伴随着新的限制来防止滥用。</p>
<p class="translated">DALL-E 在我们关于它的原始帖子中有详细的描述<a href="https://web.archive.org/web/20230408022141/https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/">，但要点是它能够接受相当复杂的提示，例如“一只熊骑着自行车穿过一个商场，旁边是一只偷《独立宣言》的猫的图片。”它会欣然接受，并从数百个输出中找到最有可能符合用户标准的。</a></p>
<p class="translated">DALL-E 2 从根本上做了同样的事情，将一个文本提示变成一个惊人准确的图像。但是它已经学会了一些新的技巧。</p>
<p class="translated">首先，它只是更擅长做原创的事情。从 DALL-E 2 另一端出来的图像要大几倍，也更加详细。它实际上更快，尽管产生了更多的图像，这意味着更多的变化可以在用户可能愿意等待的几秒钟内完成。</p>
<p/><div id="attachment_2295356" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2295356" decoding="async" class="wp-image-2295356 size-full" src="../Images/3cdae9f9ee0f4a2ccd9bfb4151b80499.png" alt="" srcset="https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg 1024w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=150,150 150w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=300,300 300w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=768,768 768w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=680,680 680w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=32,32 32w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=50,50 50w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=64,64 64w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=96,96 96w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg?resize=128,128 128w" sizes="(max-width: 1024px) 100vw, 1024px" data-original-src="https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/sea-otter-in-the-style-of-Girl-with-a-Pearl-Earring.jpg"/><p id="caption-attachment-2295356" class="wp-caption-text translated">“戴珍珠耳环的女孩风格的海獭”看起来很不错。<strong>图片来源:</strong> OpenAI</p></div>
<p class="translated">这种改善部分来自于向扩散模型的转变，这是一种从纯噪声开始并随着时间的推移不断完善图像的图像创建方式，反复使其更像所要求的图像，直到完全没有噪声。但它也只是一个更小、更高效的模型，一些从事这项工作的工程师告诉我。</p>
<p class="translated">其次，DALL-E 做了他们所谓的“修补”，本质上是对图像中给定区域的智能替换。假设你有一张你家的照片，但是桌子上有一些脏盘子。简单地选择那个区域，描述你想要的东西:“一张空的木头桌子”，或者“一张没有盘子的桌子”，任何看起来合理的东西。在几秒钟内，这个模型将向你展示该提示的几种解释，你可以选择看起来最好的解释。</p>
<p class="translated">你可能熟悉 Photoshop 中类似的东西，“上下文感知填充”但是这个工具更多的是用来填充空间的，就像如果你想在晴朗的天空中替换一只鸟，并且不想为克隆冲压而烦恼。DALL-E 2 的功能更强大，能够发明新的东西，例如一种不同的鸟，或一朵云，或者在桌子的情况下，一瓶花或一瓶洒了的番茄酱。不难想象这方面的有用应用。</p>
<p class="translated">值得注意的是，该模型将包括适当的照明和阴影，或者选择正确的材料，因为它知道场景的其余部分。我在这里不严格地使用“aware”——没有人知道 DALL-E 如何在内部表示这些概念，甚至它的创造者也不知道，但对这些目的来说，重要的是结果表明它有某种形式的理解。</p>
<p/><div id="attachment_2295373" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2295373" decoding="async" loading="lazy" class="size-full wp-image-2295373" src="../Images/91a2c0e44204b0fbbba61dfc40ae6955.png" alt="" srcset="https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg 3072w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=150,100 150w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=300,200 300w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=768,512 768w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=680,453 680w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=1536,1024 1536w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=2048,1365 2048w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=1200,800 1200w, https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg?resize=50,33 50w" sizes="(max-width: 1024px) 100vw, 1024px" data-original-src="https://web.archive.org/web/20230408022141im_/https://techcrunch.com/wp-content/uploads/2022/04/DALL-E-teddy-bears-ukiyo-e-flower-shop.jpg"/><p id="caption-attachment-2295373" class="wp-caption-text translated">浮世绘风格的泰迪熊和古色古香的花店的例子。<strong>图片来源:</strong> OpenAI</p></div>
<p class="translated">第三个新功能是“变化”，这足够准确:您给系统一个示例图像，它会根据您的需要生成许多变化，从非常接近的变化到印象主义的重做。你甚至可以给它第二个图像，它会将它们交叉授粉，结合各自最突出的方面。他们给我看的演示让 DALL-E 2 根据一幅原作生成街头壁画，它确实在很大程度上捕捉到了艺术家的风格，即使在检查时可能很清楚哪幅是原作。</p>
<p class="translated">与我见过的其他生成器相比，很难夸大这些图像的质量。虽然几乎总是有你期望从人工智能生成的图像中得到的那种“告诉”，但它们不那么明显，并且图像的其余部分比其他人生成的最佳图像好得多。</p>
<h2 class="translated">几乎任何事情</h2>
<p class="translated">我以前写过 DALL-E 2 可以画“几乎任何东西”，尽管没有任何技术限制可以阻止这个模型令人信服地画出你能想到的任何东西。但 OpenAI 意识到了 deepfakes 和其他滥用人工智能生成的图像和内容所带来的风险，因此为他们的最新模型添加了一些限制。</p>
<p class="translated">DALL-E 2 目前运行在一个托管平台上，这是一个只接受邀请的测试环境，开发人员可以在可控的方式下进行测试。这部分意味着他们对模型的所有提示都被评估是否违反了内容政策，正如他们所说，该政策禁止“非 G 级图像”</p>
<p class="translated">这意味着没有:仇恨、骚扰、暴力、自残、露骨或“令人震惊”的图像、非法活动、欺骗(例如，假新闻报道)、政治人物或情况、医疗或疾病相关图像或一般垃圾邮件。事实上，这大部分是不可能的，因为违反图像被排除在训练集之外:DALL-E 2 可以做一个戴贝雷帽的柴犬，但它甚至不知道什么是导弹袭击。</p>
<p class="translated">除了对提示进行评估之外，生成的图像将全部(目前)由人类检查员进行审查。这显然是不可扩展的，但是团队告诉我这是学习过程的一部分。他们不确定边界应该如何工作，这就是为什么他们现在保持平台的小型和自托管。</p>
<p class="translated">随着时间的推移，DALL-E 2 可能会变成一个 API，可以像 OpenAI 的其他功能一样调用，但该团队表示，他们希望在拆除训练轮之前确保这是明智的。</p>
<p class="translated">你可以在 OpenAI 博客文章中了解更多关于 DALL-E 2 的信息，并测试一些半交互式的例子<a href="https://web.archive.org/web/20230408022141/https://openai.com/dall-e-2/">。</a></p>
			</div>

			</div>    
</body>
</html>