<html>
<head>
<title>Google's image generator rivals DALL-E in shiba inu drawing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌的图像生成器在柴犬绘图方面与 DALL-E 竞争</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2022/05/23/openai-look-at-our-awesome-image-generator-google-hold-my-shiba-inu/">https://web.archive.org/web/https://techcrunch.com/2022/05/23/openai-look-at-our-awesome-image-generator-google-hold-my-shiba-inu/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">人工智能世界仍在研究如何应对 DALL-E 2 惊人的能力展示，这是 DALL-E 2 绘制/描绘/想象任何东西的能力。Google Research 匆忙公布了一个它正在研究的类似模型——它声称这个模型更好。</p>
<p class="translated"><a href="https://web.archive.org/web/20230408173938/https://gweb-research-imagen.appspot.com/" target="_blank" rel="noopener"> Imagen </a>(懂了吗？)是一个基于文本到图像扩散的生成器，它建立在大型 transformer 语言模型之上……好了，让我们慢下来，很快的解开它。</p>
<p class="translated">文本到图像模型接受文本输入，如“一只骑在自行车上的狗”,并产生相应的图像，这项工作已经做了多年，但最近在质量和可访问性方面有了巨大的飞跃。</p>
<p class="translated">其中一部分是使用扩散技术，基本上从一个纯噪声图像开始，慢慢地一点一点地完善它，直到模型认为它不能让它看起来比它已经做的更像一只骑自行车的狗。这是对自顶向下生成器的一个改进，自顶向下生成器第一次猜测可能会出错，而其他生成器很容易被引入歧途。</p>
<p class="translated">另一部分是通过使用 transformer 方法的<a href="https://web.archive.org/web/20230408173938/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/">大型语言模型</a>来提高语言理解，我不会(也不能)在这里讨论它的技术方面，但它和其他一些最近的进展已经导致了令人信服的语言模型，如 GPT-3 和其他。</p>
<p/><div id="attachment_2323704" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2323704" decoding="async" class="size-full wp-image-2323704" src="../Images/9c1b0fda12b0760ff3232b912cbb0d47.png" alt="Examples of Imagen generated art." srcset="https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg 927w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg?resize=150,113 150w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg?resize=300,226 300w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg?resize=768,577 768w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg?resize=680,511 680w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg?resize=50,38 50w" sizes="(max-width: 927px) 100vw, 927px" data-original-src="https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/options.jpg"/><p id="caption-attachment-2323704" class="wp-caption-text translated"><strong>图片来源:</strong>谷歌研究</p></div>
<p class="translated">Imagen 首先生成一个小的(64×64 像素)图像，然后对其进行两次“超分辨率”处理，使其达到 1024×1024。不过，这不像普通的放大，因为人工智能超分辨率以原始图像为基础，创造了与较小图像和谐的新细节。</p>
<p class="translated">比方说，你有一只骑自行车的狗，在第一张图片中，狗的眼睛有 3 个像素宽。没有太多的表达空间！但是在第二张图片上，它是 12 像素宽。这需要的细节来自哪里？嗯，人工智能知道狗的眼睛是什么样的，所以它在绘制时会生成更多的细节。然后，当眼睛再次完成时，这种情况再次发生，但在 48 像素的范围内。但是在任何时候，人工智能都不需要从它的狗眼里拿出 48 像素的东西…让我们说是魔法包吧。像许多艺术家一样，它从相当于一个粗略的草图开始，在一个研究中完成，然后真正在最终的画布上完成。</p>
<p class="translated">这并不是前所未有的，事实上，使用人工智能模型的艺术家已经使用这种技术来创作比人工智能一次处理的作品大得多的作品。如果你把一张画布分成几块，然后分别对它们进行超分辨率处理，你最终会得到更大更复杂的细节；甚至可以反复做。<a href="https://web.archive.org/web/20230408173938/https://twitter.com/dilkROMGlitches/status/1526389920033644545" target="_blank" rel="noopener">一个有趣的例子</a>来自我认识的一位艺术家:</p>

<p class="translated">谷歌的研究人员声称 Imagen 有几项进步。他们说，现有的文本模型可以用于文本编码部分，它们的质量比简单地增加视觉保真度更重要。这在直觉上是有道理的，因为一张详细的胡说八道的图片肯定比一张稍微不太详细的你所要求的图片更糟糕。</p>
<p class="translated">例如，在描述 Imagen 的<a href="https://web.archive.org/web/20230408173938/https://gweb-research-imagen.appspot.com/paper.pdf" target="_blank" rel="noopener">论文</a>中，他们比较了 Imagen 和 DALL-E 2 做“熊猫制作拿铁艺术”的结果。在后者的所有图像中，都是熊猫的拿铁艺术；在 Imagen 的大部分作品中，是一只熊猫在创造艺术。(两者都无法描绘骑马的宇航员，在所有的尝试中都表现出相反的情况。这是一项正在进行的工作。)</p>
<p/><div id="attachment_2323700" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-2323700" decoding="async" loading="lazy" class="size-full wp-image-2323700" src="../Images/d6c652b9b526c6fbdf96d3fd6d657806.png" alt="Computer-generated images of pandas making or being latte art." srcset="https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg 1112w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg?resize=150,81 150w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg?resize=300,163 300w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg?resize=768,416 768w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg?resize=680,369 680w, https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg?resize=50,27 50w" sizes="(max-width: 1024px) 100vw, 1024px" data-original-src="https://web.archive.org/web/20230408173938im_/https://techcrunch.com/wp-content/uploads/2022/05/panda-art.jpg"/><p id="caption-attachment-2323700" class="wp-caption-text translated"><strong>图片来源:</strong>谷歌研究</p></div>
<p class="translated">在谷歌的测试中，Imagen 在人类评估测试中脱颖而出，无论是准确性还是保真度。这显然是非常主观的，但即使是与 DALL-E 2 的感知质量相匹配，也是相当令人印象深刻的，直到今天，DALL-E2 仍被认为是超越一切的巨大飞跃。我只想补充一点，虽然它很好，但这些图像(来自任何生成器)都无法在人们注意到它们是生成的或产生严重怀疑之前经受住粗略的审查。</p>
<p class="translated">不过，OpenAI 在几个方面领先谷歌一两步。DALL-E 2 不仅仅是一篇研究论文，它是一个私人测试版，人们使用它，就像他们使用它的前身和 GPT-2 和 3 一样。具有讽刺意味的是，以“open”命名的公司一直专注于文本到图像研究的产品化，而这个利润丰厚的互联网巨头还没有尝试过。</p>

<p class="translated">从 DALL-E 2 的研究人员做出的选择来看，这一点非常明显，他们提前整理了训练数据集，并删除了任何可能违反他们自己准则的内容。这种模式不可能做出 NSFW 那样的东西。然而，谷歌的团队使用了一些已知包含不适当材料的大型数据集。在 Imagen 网站上描述“局限性和社会影响”的深刻章节中，研究人员写道:</p>
<blockquote><p class="translated">文本到图像模型的下游应用多种多样，并可能以复杂的方式影响社会。滥用的潜在风险引发了对负责任的代码和演示开源的关注。在这个时候，我们已经决定不发布代码或公开演示。</p>
<p class="translated">文本到图像模型的数据需求导致研究人员严重依赖于大型的、大多未经删节的网络搜集的数据集。虽然这种方法近年来实现了快速的算法进步，但这种性质的数据集往往反映了社会刻板印象、压迫性观点以及对边缘化身份群体的贬损或有害联系。虽然我们过滤了一部分训练数据，以删除噪音和不良内容，如色情图像和有毒语言，但我们还利用了 LAION-400M 数据集，该数据集已知包含广泛的不良内容，包括色情图像、种族主义诽谤和有害的社会刻板印象。Imagen 依赖于在未切割的网络规模数据上训练的文本编码器，因此继承了大型语言模型的社会偏见和局限性。因此，Imagen 有可能编码了有害的刻板印象和表述，这促使我们决定在没有进一步安全措施的情况下不公开发布 Imagen</p></blockquote>
<p class="translated">虽然有些人可能会对此吹毛求疵，说谷歌担心它的人工智能可能在政治上不够正确，但这是一种苛刻和短视的观点。人工智能模型的好坏取决于它接受训练的数据，并不是每个团队都可以花费时间和精力来删除这些刮刀在收集数百万张图像或数十亿字数据集时收集的真正可怕的东西。</p>
<p class="translated">这种偏见意味着在研究过程中出现，这暴露了系统是如何工作的，并为识别这些和其他限制提供了一个不受约束的测试场地。否则我们怎么知道人工智能不能画出黑人中常见的发型——任何孩子都能画的发型？或者当被提示写关于工作环境的故事时，人工智能总是让老板变成男人？在这些情况下，一个人工智能模型按照设计完美地工作——它成功地学习了遍布它所训练的媒体的偏见。不是不像人！</p>
<p class="translated">但是，尽管对许多人来说，消除系统性偏见是一个终身的项目，但人工智能却更容易，它的创造者可以删除最初导致它行为不端的内容。也许有一天会需要人工智能以 50 年代种族主义、性别歧视的学者的风格写作，但目前来看，包含这些数据的好处很小，风险很大。</p>
<p class="translated">无论如何，Imagen 和其他公司一样，显然仍处于实验阶段，除了严格的人工监督方式之外，还没有准备好投入使用。当谷歌开始让它的功能变得更容易获取的时候，我确信我们会更多地了解它是如何工作的以及为什么工作的。</p>

			</div>

			</div>    
</body>
</html>